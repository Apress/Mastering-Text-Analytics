{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e47f07-5898-4a3b-b2c2-d77a59e09aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" In the following demo program, you may find many unknown terms. We have tried to explain them\n",
    "within the program itself. No worries but! We will take up all of it in the below sections.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea26ef9-82a7-4091-8bdd-cf8fbf721f1d",
   "metadata": {},
   "source": [
    "### Building a Dynamic Chat Assistant with LangChain and OpenAI's Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170b234-487c-49c8-bbde-892543292529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This program shows how to use LangChain and OpenAI's chat models to create a smart assistant. \n",
    "It combines user input with instructions for the AI to give helpful answers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b30e34-568e-485f-b636-4a6cd0d7ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could store the openai key in the form of a string in a .txt file\n",
    "# This way of storing the API key is not very secure, but suitable for your personal projects on local computers\n",
    "# Below example will make it more clear for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539af8f-416e-4946-b9fe-db0c27c35079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the essential libraries if not done already\n",
    "# !pip install openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "202701a1-936b-499e-bba2-b88e920aa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:\\\\Users\\\\Shailendra Kadre\\\\Desktop\\\\OPEN_AI_KEY.txt')\n",
    "api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af927685-0abd-4f88-b241-93588231c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of api_key is copied correctly\n",
    "# api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36b44da3-06c3-43e0-b568-dd0a60053910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: Our solar system consists of the Sun, eight planets (Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune), and various smaller celestial objects like moons, asteroids, and comets. It is located in the Milky Way galaxy.\n",
      "\n",
      "One other well-known solar system is the TRAPPIST-1 system. TRAPPIST-1 is a star system located about 39 light-years away from Earth. It is known for having seven Earth-sized planets, three of which are located in the habitable zone where conditions might be right for liquid water to exist on the surface. This system is smaller and cooler than our Sun, and its planets are much closer to their star compared to the planets in our solar system.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nStep 7: Display the AI Response\\n- Extracts the generated text (from the `content` attribute of `response`).\\n- Prints the AI's response to the console for the user to review.\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Import Necessary Modules\n",
    "- `ChatOpenAI`: Enables interaction with OpenAI's language models like GPT-3.5 Turbo.\n",
    "- `ChatPromptTemplate`: Combines system and human message templates into a conversation format.\n",
    "- `HumanMessagePromptTemplate`: Represents the user's input.\n",
    "- `SystemMessagePromptTemplate`: Represents instructions for the model, such as its role or behavior.\n",
    "- `AIMessage`, `HumanMessage`, `SystemMessage`: Define different types of messages exchanged in the chat.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the Chat Model\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=api_key)\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Initialize the Chat Model\n",
    "- `model`: Specifies the OpenAI model to use (e.g., `gpt-3.5-turbo`).\n",
    "- `temperature`: Controls the randomness of the model's output (lower = more deterministic).\n",
    "- `openai_api_key`: Passes the OpenAI API key to authenticate requests.\n",
    "\"\"\"\n",
    "\n",
    "# Define the Chat Prompt Template\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that provides concise and accurate answers.\"\n",
    ")\n",
    "\"\"\"\n",
    "Step 3a: Define the System Message\n",
    "- This message provides context or instructions to the AI, guiding its behavior.\n",
    "- Here, the AI is instructed to act as a helpful and concise assistant.\n",
    "\"\"\"\n",
    "\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\"\"\"\n",
    "Step 3b: Define the Human Message\n",
    "- A placeholder `{user_input}` represents the user's input dynamically.\n",
    "- It will later be replaced with actual text provided by the user.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\"\"\"\n",
    "Step 3c: Combine the Messages\n",
    "- Combines `system_message` and `human_message` into a single structured prompt.\n",
    "- This ensures the AI has instructions for the user's query.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare User Input\n",
    "user_input = \"Explain the difference between our solar system and one other well known one\"\n",
    "\"\"\"\n",
    "Step 4: Define the User Input\n",
    "- The user's query is stored in a variable to dynamically populate the human message template.\n",
    "\"\"\"\n",
    "\n",
    "# Format the Prompt\n",
    "messages = chat_prompt.format_messages(user_input=user_input)\n",
    "\"\"\"\n",
    "Step 5: Format the Prompt\n",
    "- Replaces `{user_input}` in the human message template with the actual user query.\n",
    "- Creates a structured message format combining the system and human messages.\n",
    "\"\"\"\n",
    "\n",
    "# Get Model Response\n",
    "response = chat_model(messages)\n",
    "\"\"\"\n",
    "Step 6: Get the AI's Response\n",
    "- Sends the formatted prompt to the model using the `chat_model` instance.\n",
    "- Receives the AI's response, stored in the `response` variable.\n",
    "\"\"\"\n",
    "\n",
    "# Display the AI Response\n",
    "print(\"AI Response:\", response.content)\n",
    "\"\"\"\n",
    "Step 7: Display the AI Response\n",
    "- Extracts the generated text (from the `content` attribute of `response`).\n",
    "- Prints the AI's response to the console for the user to review.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7429302-a2ac-4e1c-a4d0-3a1647cf9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We use gpt-3.5-turbo because it provides a balance between cost, speed, and quality. It is optimized for efficiency, \n",
    "making it ideal for real-time applications and widely adopted tasks that require robust yet economical solutions.\n",
    "\n",
    "Other popular OpenAI models include gpt-4 for advanced reasoning, text-davinci-003 for high-quality outputs, \n",
    "and smaller models like curie, babbage, and ada for simpler tasks. Each model is tailored for specific needs, \n",
    "from cost-saving to handling complex scenarios. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9c5d3-bcab-4534-99f4-a555fc966b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
