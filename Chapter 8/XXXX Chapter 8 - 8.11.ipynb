{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f479428-4564-4dcf-bc93-6a31267b4f99",
   "metadata": {},
   "source": [
    "### Parsing and Serialization Using LangChain and OpenAI (using JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2229c-1848-470c-b215-7202116f2963",
   "metadata": {},
   "source": [
    "This program shows how to use JSON to save and load prompts with PromptTemplate, along with parsing input, interacting with OpenAI, and saving the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b1d4ac-149a-4ff3-bcd8-8b735202d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the open ai api key from your text filr\n",
    "f = open('C:\\\\Users\\\\Shailendra Kadre\\\\Desktop\\\\OPEN_AI_KEY.txt')\n",
    "api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293757cf-b732-499a-9b4f-f28ff8f15f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt saved to saved_prompt.json\n",
      "Loaded Prompt: Question: {question}\n",
      "Answer: {answer}\n",
      "Parsed Query: ['what', 'is', 'generative', 'ai?']\n",
      "Response saved to response.json\n",
      "Model Response: {'question': 'What is Generative AI?', 'answer': 'Generative AI is a field of AI that generates new content.', 'text': ' This can include text, images, music, and more. Generative AI systems are trained on large datasets and use algorithms to create new, original content based on that training data. This technology has applications in a wide range of industries, from creating art and music to generating realistic images and videos.'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from langchain\n",
    "from langchain_openai import ChatOpenAI  # Use the updated class from langchain_openai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain  # Use LLMChain (still supported, with updated methods)\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI's Chat model (updated method)\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=api_key)\n",
    "\n",
    "# Define a simple prompt template\n",
    "template = \"Question: {question}\\nAnswer: {answer}\"\n",
    "prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=template)\n",
    "\n",
    "# Save the prompt template to a JSON file\n",
    "prompt_data = {\n",
    "    \"input_variables\": prompt.input_variables,\n",
    "    \"template\": prompt.template\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"saved_prompt.json\", \"w\") as json_file:\n",
    "    json.dump(prompt_data, json_file)\n",
    "print(\"Prompt saved to saved_prompt.json\")\n",
    "\n",
    "# Load the saved prompt template from the JSON file\n",
    "with open(\"saved_prompt.json\", \"r\") as json_file:\n",
    "    loaded_prompt_data = json.load(json_file)\n",
    "\n",
    "# Recreate the prompt template using the loaded data\n",
    "loaded_prompt = PromptTemplate(input_variables=loaded_prompt_data[\"input_variables\"], \n",
    "                               template=loaded_prompt_data[\"template\"])\n",
    "print(f\"Loaded Prompt: {loaded_prompt.template}\")\n",
    "\n",
    "# Create an LLMChain to link the prompt with the OpenAI model\n",
    "llm_chain = LLMChain(prompt=loaded_prompt, llm=llm)\n",
    "\n",
    "# **Parsing Example**: Breaking down the user input into a structured format\n",
    "query = \"What is Generative AI?\"\n",
    "parsed_query = query.lower().split()  # Convert query to lowercase and split into words\n",
    "print(f\"Parsed Query: {parsed_query}\")  # Output: ['what', 'is', 'generative', 'ai']\n",
    "\n",
    "# Prepare the response by feeding the parsed query into the model\n",
    "response = llm_chain.invoke({\"question\": query, \"answer\": \"Generative AI is a field of AI that generates new content.\"})\n",
    "\n",
    "# **Serialization Example**: Saving the model's response into a file\n",
    "serialized_response = json.dumps({\"question\": query, \"answer\": response})  # Convert to JSON format\n",
    "with open(\"response.json\", \"w\") as file:\n",
    "    json.dump({\"question\": query, \"answer\": response}, file)  # Save to file\n",
    "print(\"Response saved to response.json\")\n",
    "\n",
    "# Print the response\n",
    "print(f\"Model Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf7e05-9e83-4ebc-b835-ad39efb71e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
