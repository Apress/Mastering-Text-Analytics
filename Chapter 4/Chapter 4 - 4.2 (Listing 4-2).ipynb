{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b487067",
   "metadata": {},
   "source": [
    "- Tokenization using nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fa22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk if you have not done it already.\n",
    "!pip install nltk # The code output not included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80690f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['We', 'had', 'already', 'discussed', 'about', 'tokenisation', 'in', 'RegEx', 'sections', '.', 'Tokenization', 'is', 'a', 'vital', 'process', 'in', 'NLP', 'data', 'pre-processing', '.', 'It', 'involves', 'breaking', 'down', 'larger', 'text', 'data', 'into', 'minor', 'units', 'of', 'words', 'or', 'sentences', '.', 'This', 'operation', 'further', 'enables', 'the', 'identification', 'of', 'meaningful', 'elements', 'within', 'the', 'larger', 'text', 'chunks', '.']\n",
      "\n",
      "Sentence Tokens: ['\\nWe had already discussed about tokenisation in RegEx sections.', 'Tokenization is a vital process in NLP data pre-processing.', 'It involves breaking down larger text data into minor units of words or sentences.', 'This operation further enables the identification of meaningful elements within the larger text chunks.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Create sample text.\n",
    "sample_text = \"\"\"\n",
    "We had already discussed about tokenisation in RegEx sections. \n",
    "Tokenization is a vital process in NLP data pre-processing. \n",
    "It involves breaking down larger text data into minor units of words or sentences. \n",
    "This operation further enables the identification of meaningful elements within the larger text chunks. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Perform word level tokenization first. \n",
    "words = word_tokenize(sample_text)\n",
    "print(\"Word Tokens:\", words)\n",
    "\n",
    "# Then perform sentence level tokenization. \n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"\\nSentence Tokens:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e4cf3",
   "metadata": {},
   "source": [
    "Code Snippet 13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
