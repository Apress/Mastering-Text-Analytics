{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b487067",
   "metadata": {},
   "source": [
    "- Stop words removal using nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fa22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk if you have not done it already.\n",
    "!pip install nltk # The code output not included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7f6687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shailendra\n",
      "[nltk_data]     Kadre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Shailendra\n",
      "[nltk_data]     Kadre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Important required libaries. \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure required resources.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def stopwords_remover(text, language='english'):\n",
    "    \"\"\"\n",
    "    This functions filters out stop words from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The Sample text to remove stop words.\n",
    "        language (str): The target language. \n",
    "\n",
    "    Returns:\n",
    "        list: A list of words with stop words removed.\n",
    "    \"\"\"\n",
    "    # Tokenize the text.\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Retrieve the list of stop words for English.\n",
    "    stop_words = set(stopwords.words(language))\n",
    "\n",
    "    # Remove stop words.\n",
    "    ouput_text = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    return ouput_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0952523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['already', 'discussed', 'tokenisation', 'RegEx', 'sections', '.', 'Tokenization', 'vital', 'process', 'NLP', 'data', 'pre-processing', '.', 'involves', 'breaking', 'larger', 'text', 'data', 'minor', 'units', 'words', 'sentences', '.', 'operation', 'enables', 'identification', 'meaningful', 'elements', 'within', 'larger', 'text', 'chunks', '.']\n"
     ]
    }
   ],
   "source": [
    "# We will keep the sample text same as the tokanisation example. \n",
    "# This is to show the difference between tokanisation and stop words removal. \n",
    "sample_text = \"\"\"\n",
    "We had already discussed about tokenisation in RegEx sections. \n",
    "Tokenization is a vital process in NLP data pre-processing. \n",
    "It involves breaking down larger text data into minor units of words or sentences. \n",
    "This operation further enables the identification of meaningful elements within the larger text chunks. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Filter out stop words.\n",
    "filtered_words = stopwords_remover(sample_text)\n",
    "\n",
    "# Output the results.\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e4cf3",
   "metadata": {},
   "source": [
    "Code Snippet 14"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
