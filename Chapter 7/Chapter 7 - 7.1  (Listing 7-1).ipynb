{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b09633",
   "metadata": {},
   "source": [
    "#### Demostrating coreference resolution, anaphora resolution, coherence modelling, rhetorical structure theory (RST), discourse parsing, entity tracking, lexical chains, topic modelling, cohesion analysis, and temporal relation identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6be321-f64b-4050-aebb-ed9b69354daf",
   "metadata": {},
   "source": [
    "#### Coreference resolution (code demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ae282-18fd-4409-bd8d-9525ca383365",
   "metadata": {},
   "source": [
    "- Most of these techniques use popular NLP libraries in Python, such as spaCy, NLTK, Transformers, Gensim, and others.\n",
    "- Here's a brief idea of how each technique can be demonstrated with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab62ae-69be-4689-93e8-0921700b7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package needed for coreference resolution.\n",
    "# Uncomment it if coreferee is not installed on your system.\n",
    "!python -m pip install coreferee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df1847-d758-4a7c-a301-d3f698c4d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the English coreference resolution model for the coreferee package.\n",
    "!python -m coreferee install en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cab6b4-3f3a-4f55-a597-200c723de701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the large English model ('en_core_web_lg') for spaCy.\n",
    "# This model includes word vectors and is more accurate for NER and POS tagging.\n",
    "# It's larger and more powerful than the smaller models lke 'en_core_web_sm'), but it may take up large memory.\n",
    "# We have not loaded it in the main code body but without this step, the code was throwing error.\n",
    "# So it's a required step. \n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f7d938f-9618-4b83-a869-c1438e78d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to ignore warnings. \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13fa71d2-02b1-40a5-a70f-eea013f35879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUTPUT \n",
      "\n",
      "0: she(2), Mary(10), she(12), She(20), her(22)\n",
      "1: work(8), it(17)\n",
      "2: [She(20); spouse(23)], they(25), They(34), they(41)\n",
      "3: France(39), country(47)\n"
     ]
    }
   ],
   "source": [
    "# Import the required package. \n",
    "import spacy\n",
    "# Load the transformer-based English model.\n",
    "# As of now let's use it. We will take it up in detail in the later chapters. \n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "\"\"\" \n",
    "Add 'coreferee' to the pipeline.\n",
    "'coreferee' is a spaCy extension that enables the identification and linking of coreferences in a text.\n",
    "Adding it to the pipeline so that coreference resolution can be performed after other NLP tasks like tokenization and parsing.\n",
    "\"\"\"\n",
    "nlp.add_pipe('coreferee')\n",
    "# Create the sample text for demo. \n",
    "sample_doc = nlp(\"\"\" Although she was very busy at office work, Mary felt she had had enough of it. \n",
    "                     She and her spouse decided they needed to go on a holiday. \n",
    "                     They travelled by train to France because they had enough friends in the country.\"\"\")\n",
    "print(\"\")\n",
    "print(\"OUTPUT \\n\")\n",
    "sample_doc._.coref_chains.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822ac11-f9d7-4b9c-a809-2fd6dcb0c7e9",
   "metadata": {},
   "source": [
    "The output above does not look that easy to understand. The first line of the production (indexed 0) tells us that the pronouns, she(2), she(12), She(20), and her(22) refer to the same name, 'Mary(10).' Similarly, the second line (indexed 1) makes us understand that it(17) stands for work(8). At index 2 position, they(25), They(34), they(41) refer to [She(20); spouse(23)]. Finally, at index 3, country(47) stands for France(39). You will notice that in this output all the pronouns in the sample_doc are resolved to their proper nouns (coreference resolution process)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46926d-f8aa-4834-8780-5fe933c68bfa",
   "metadata": {},
   "source": [
    "#### Rhetorical structure theory (RST) (code demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb120e-485d-48f4-b290-1f6547fc674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "932f9aa9-47e4-49dc-b5c3-140e465baed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1022cc610b04d679f12c5211b381398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 16:28:37 INFO: Downloaded file to C:\\Users\\Shailendra Kadre\\stanza_resources\\resources.json\n",
      "2024-09-08 16:28:37 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-09-08 16:28:38 INFO: File exists: C:\\Users\\Shailendra Kadre\\stanza_resources\\en\\default.zip\n",
      "2024-09-08 16:28:40 INFO: Finished downloading models and saved to C:\\Users\\Shailendra Kadre\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download and set up the Stanford NLP mode.\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bd73a81-0271-43a7-8f3b-cce0397c0619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 16:30:35 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa69eecc06847fc9c18c0cbd0e1c88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 16:30:36 INFO: Downloaded file to C:\\Users\\Shailendra Kadre\\stanza_resources\\resources.json\n",
      "2024-09-08 16:30:36 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-09-08 16:30:36 INFO: Using device: cpu\n",
      "2024-09-08 16:30:36 INFO: Loading: tokenize\n",
      "2024-09-08 16:30:36 INFO: Loading: mwt\n",
      "2024-09-08 16:30:36 INFO: Loading: pos\n",
      "2024-09-08 16:30:36 INFO: Loading: lemma\n",
      "2024-09-08 16:30:36 INFO: Loading: depparse\n",
      "2024-09-08 16:30:37 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Stanza Pipeline for processing English text.\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,depparse, lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be06f3dd-cc2b-4965-a2a4-fd68d6763fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Your', 'example', 'sentence', 'goes', 'here', '.']\n",
      "POS Tags: ['PRON', 'NOUN', 'NOUN', 'VERB', 'ADV', 'PUNCT']\n",
      "Dependencies: [('Your', 'nmod:poss'), ('example', 'compound'), ('sentence', 'nsubj'), ('goes', 'root'), ('here', 'advmod'), ('.', 'punct')]\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Your example sentence goes here.\")\n",
    "for sentence in doc.sentences:\n",
    "    print(\"Tokens:\", [word.text for word in sentence.words])\n",
    "    print(\"POS Tags:\", [word.pos for word in sentence.words])\n",
    "    print(\"Dependencies:\", [(word.text, word.deprel) for word in sentence.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68233c0c-0c61-43b2-8b82-31ca3e62ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Although she was very busy at office work , Mary felt she had had enough of it .\n",
      "Tokens: ['Although', 'she', 'was', 'very', 'busy', 'at', 'office', 'work', ',', 'Mary', 'felt', 'she', 'had', 'had', 'enough', 'of', 'it', '.']\n",
      "POS Tags: ['SCONJ', 'PRON', 'AUX', 'ADV', 'ADJ', 'ADP', 'NOUN', 'NOUN', 'PUNCT', 'PROPN', 'VERB', 'PRON', 'AUX', 'VERB', 'ADJ', 'ADP', 'PRON', 'PUNCT']\n",
      "Dependencies: [('Although', 'mark'), ('she', 'nsubj'), ('was', 'cop'), ('very', 'advmod'), ('busy', 'advcl'), ('at', 'case'), ('office', 'compound'), ('work', 'obl'), (',', 'punct'), ('Mary', 'nsubj'), ('felt', 'root'), ('she', 'nsubj'), ('had', 'aux'), ('had', 'ccomp'), ('enough', 'obj'), ('of', 'case'), ('it', 'obl'), ('.', 'punct')]\n",
      "-----\n",
      "Sentence: She and her spouse decided they needed to go on a holiday .\n",
      "Tokens: ['She', 'and', 'her', 'spouse', 'decided', 'they', 'needed', 'to', 'go', 'on', 'a', 'holiday', '.']\n",
      "POS Tags: ['PRON', 'CCONJ', 'PRON', 'NOUN', 'VERB', 'PRON', 'VERB', 'PART', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "Dependencies: [('She', 'nsubj'), ('and', 'cc'), ('her', 'nmod:poss'), ('spouse', 'conj'), ('decided', 'root'), ('they', 'nsubj'), ('needed', 'ccomp'), ('to', 'mark'), ('go', 'xcomp'), ('on', 'case'), ('a', 'det'), ('holiday', 'obl'), ('.', 'punct')]\n",
      "-----\n",
      "Sentence: They travelled by train to France because they had enough friends in the country .\n",
      "Tokens: ['They', 'travelled', 'by', 'train', 'to', 'France', 'because', 'they', 'had', 'enough', 'friends', 'in', 'the', 'country', '.']\n",
      "POS Tags: ['PRON', 'VERB', 'ADP', 'NOUN', 'ADP', 'PROPN', 'SCONJ', 'PRON', 'VERB', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "Dependencies: [('They', 'nsubj'), ('travelled', 'root'), ('by', 'case'), ('train', 'obl'), ('to', 'case'), ('France', 'obl'), ('because', 'mark'), ('they', 'nsubj'), ('had', 'advcl'), ('enough', 'amod'), ('friends', 'obj'), ('in', 'case'), ('the', 'det'), ('country', 'nmod'), ('.', 'punct')]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Create the sample text for demo. \n",
    "sample_doc = nlp(\"\"\" Although she was very busy at office work, Mary felt she had had enough of it. \n",
    "                     She and her spouse decided they needed to go on a holiday. \n",
    "                     They travelled by train to France because they had enough friends in the country.\"\"\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(sample_doc)\n",
    "\n",
    "# Print out tokens, POS tags, and dependency relations.\n",
    "for sentence in doc.sentences:\n",
    "    print(\"Sentence:\", \" \".join([word.text for word in sentence.words]))\n",
    "    print(\"Tokens:\", [word.text for word in sentence.words])\n",
    "    print(\"POS Tags:\", [word.pos for word in sentence.words])\n",
    "    print(\"Dependencies:\", [(word.text, word.deprel) for word in sentence.words])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e0ea1-25e0-4d92-bf35-d34fe4d91467",
   "metadata": {},
   "source": [
    "We will take the middle sentence for elaborations on the output.\n",
    "\n",
    "Sentence\"She and her spouse decided they needed to go on a holiday.\"\n",
    "\n",
    "Tokens and POS Tags:\n",
    "Tokens: ['She', 'and', 'her', 'spouse', 'decided', 'they', 'needed', 'to', 'go', 'on', 'a', 'holiday', '.']\n",
    "POS Tags: ['PRON', 'CCONJ', 'PRON', 'NOUN', 'VERB', 'PRON', 'VERB', 'PART', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
    "PRON (Pronoun): 'She', 'her', 'they'\n",
    "CCONJ (Coordinating Conjunction): 'and'\n",
    "NOUN (Noun): 'spouse', 'holiday'\n",
    "VERB (Verb): 'decided', 'needed', 'go'\n",
    "PART (Particle): 'to'\n",
    "ADP (Adposition): 'on'\n",
    "DET (Determiner): 'a'\n",
    "PUNCT (Punctuation): '.'\n",
    "\n",
    "Dependencies:\n",
    "('She', 'nsubj'): 'She' is the nominal subject of the main verb 'decided'.\n",
    "('and', 'cc'): 'and' is a coordinating conjunction linking 'She' and 'her spouse'.\n",
    "('her', 'nmod:poss'): 'her' is a possessive modifier for 'spouse'.\n",
    "('spouse', 'conj'): 'spouse' is a conjunct connected to 'She' by 'and'.\n",
    "('decided', 'root'): 'decided' is the main verb (root) of the sentence.\n",
    "('they', 'nsubj'): 'they' is the subject of the embedded verb 'needed'.\n",
    "('needed', 'ccomp'): 'needed' is a clausal complement of 'decided'.\n",
    "('to', 'mark'): 'to' is a marker for the infinitive verb 'go'.\n",
    "('go', 'xcomp'): 'go' is an open clausal complement of 'needed'.\n",
    "('on', 'case'): 'on' is a preposition marking the case of 'holiday'.\n",
    "('a', 'det'): 'a' is a determiner for 'holiday'.\n",
    "('holiday', 'obl'): 'holiday' is the oblique object of the preposition 'on'.\n",
    "('.', 'punct'): '.' is punctuation marking the end of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597eaad4-cf68-44ba-86ac-398c5494c977",
   "metadata": {},
   "source": [
    "Having this information in your folds, you can now apply RST principles and manually complete the the process of RST. More advanced tools are available to complete the RST process for you. Stanza provides valuable syntactic information. However, an end-to-end RST analysis would need additional steps or tools to explicitly categorize and recognise the rhetorical relationships between different text parts. To this date, a Python library for RST is unavailable to our knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa17339-a0bb-4bd2-8dca-5b917cb86009",
   "metadata": {},
   "source": [
    "#### Discourse parsing (code demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2368a86-db0a-46b6-a607-657a791cf02e",
   "metadata": {},
   "source": [
    "A direct discourse parsing library is currently unavailable in Python. Professionals who write code often use advanced NLP libraries like spaCy or AllenNLP, along with deep learning techniques. We will stick to spaCy for a simplified demo. We will base our code on the following logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119ec6c-6c77-4a90-a720-5160824a6ad9",
   "metadata": {},
   "source": [
    "1. Load SpaCy Model\n",
    "   - Initialize SpaCy's English language model\n",
    "\n",
    "2. Define Sample Text\n",
    "   - Set a string variable with the sample text\n",
    "\n",
    "3. Process Text\n",
    "   - Use SpaCy to analyze the sample text\n",
    "   - Split the text into sentences\n",
    "\n",
    "4. Define Function to Extract Discourse Information\n",
    "   - For each sentence in the processed text:\n",
    "     a. Print the sentence\n",
    "     b. Extract and print named entities (if any)\n",
    "     c. Extract and print syntactic dependencies for each token\n",
    "     d. Infer basic discourse relations based on keywords:\n",
    "        - If the sentence contains the word \"because\":\n",
    "          - Record that this sentence provides a reason for the previous sentence\n",
    "        - If the sentence contains the word \"although\":\n",
    "          - Record that this sentence contrasts with the previous sentence\n",
    "\n",
    "5. Call Function to Extract and Display Discourse Information\n",
    "   - Print inferred discourse relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9cefe-0a64-4b2c-b299-3119352b3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f39327a-b735-4be8-bc32-d18c65b832b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: \n",
      "\n",
      "  Dependencies: [('\\n', 'dep', '\\n')]\n",
      "Sentence 2: Although she was very busy with office work, Mary felt she had had enough of it. \n",
      "\n",
      "  Named Entities: [('Mary', 'PERSON')]\n",
      "  Dependencies: [('Although', 'mark', 'was'), ('she', 'nsubj', 'was'), ('was', 'advcl', 'felt'), ('very', 'advmod', 'busy'), ('busy', 'acomp', 'was'), ('with', 'prep', 'busy'), ('office', 'compound', 'work'), ('work', 'pobj', 'with'), (',', 'punct', 'felt'), ('Mary', 'nsubj', 'felt'), ('felt', 'ROOT', 'felt'), ('she', 'nsubj', 'had'), ('had', 'aux', 'had'), ('had', 'ccomp', 'felt'), ('enough', 'dobj', 'had'), ('of', 'prep', 'enough'), ('it', 'pobj', 'of'), ('.', 'punct', 'felt'), ('\\n', 'dep', '.')]\n",
      "Sentence 3: She and her spouse decided they needed to go on a holiday. \n",
      "\n",
      "  Named Entities: [('a holiday', 'DATE')]\n",
      "  Dependencies: [('She', 'nsubj', 'decided'), ('and', 'cc', 'She'), ('her', 'poss', 'spouse'), ('spouse', 'conj', 'She'), ('decided', 'ROOT', 'decided'), ('they', 'nsubj', 'needed'), ('needed', 'ccomp', 'decided'), ('to', 'aux', 'go'), ('go', 'xcomp', 'needed'), ('on', 'prep', 'go'), ('a', 'det', 'holiday'), ('holiday', 'pobj', 'on'), ('.', 'punct', 'decided'), ('\\n', 'dep', '.')]\n",
      "Sentence 4: They travelled by train to France because they had enough friends in the country.\n",
      "\n",
      "  Named Entities: [('France', 'GPE')]\n",
      "  Dependencies: [('They', 'nsubj', 'travelled'), ('travelled', 'ROOT', 'travelled'), ('by', 'prep', 'travelled'), ('train', 'pobj', 'by'), ('to', 'prep', 'travelled'), ('France', 'pobj', 'to'), ('because', 'mark', 'had'), ('they', 'nsubj', 'had'), ('had', 'advcl', 'travelled'), ('enough', 'amod', 'friends'), ('friends', 'dobj', 'had'), ('in', 'prep', 'friends'), ('the', 'det', 'country'), ('country', 'pobj', 'in'), ('.', 'punct', 'travelled'), ('\\n', 'dep', '.')]\n",
      "\n",
      "Inferred Discourse Relations:\n",
      "Sentence 2 contrasts with Sentence 1\n",
      "Sentence 4 provides a reason for Sentence 3\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy's English model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample text.\n",
    "text = \"\"\"\n",
    "Although she was very busy with office work, Mary felt she had had enough of it. \n",
    "She and her spouse decided they needed to go on a holiday. \n",
    "They travelled by train to France because they had enough friends in the country.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text with SpaCy.\n",
    "doc = nlp(text)\n",
    "\n",
    "# Function to extract and display discourse-like information\n",
    "def extract_discourse_info(doc):\n",
    "    sentences = list(doc.sents)\n",
    "    relations = []\n",
    "\n",
    "    for i, sent in enumerate(sentences):\n",
    "        print(f\"Sentence {i+1}: {sent.text}\")\n",
    "\n",
    "        # Extract named entities.\n",
    "        entities = [(ent.text, ent.label_) for ent in sent.ents]\n",
    "        if entities:\n",
    "            print(\"  Named Entities:\", entities)\n",
    "\n",
    "        # Extract syntactic dependencies\n",
    "        dependencies = [(token.text, token.dep_, token.head.text) for token in sent]\n",
    "        print(\"  Dependencies:\", dependencies)\n",
    "\n",
    "        # Basic inference of discourse relations.\n",
    "        if i > 0:\n",
    "            previous_sent = sentences[i-1]\n",
    "            if \"because\" in sent.text.lower():\n",
    "                relations.append(f\"Sentence {i+1} provides a reason for Sentence {i}\")\n",
    "            elif \"although\" in sent.text.lower():\n",
    "                relations.append(f\"Sentence {i+1} contrasts with Sentence {i}\")\n",
    "\n",
    "    return relations\n",
    "\n",
    "# Extract and print discourse information.\n",
    "relations = extract_discourse_info(doc)\n",
    "print(\"\\nInferred Discourse Relations:\")\n",
    "for relation in relations:\n",
    "    print(relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c325e-faf7-4eb1-876c-639f0d00ce6b",
   "metadata": {},
   "source": [
    "Discourse parsing deals with how different parts of a text are related to one another. It can be through logical and communicative connections.The main part of the output is the \"discourse information.\" In the following input text, \n",
    "\n",
    "Below is the input text, labelled by sentence numbers: \n",
    "(Sentence 1) Although she was very busy with office work, (Sentence 2) Mary felt she had had enough of it. \n",
    "(Sentence 3) She and her spouse decided they needed to go on a holiday. \n",
    "(Sentence 4) They travelled by train to France because they had enough friends in the country.\n",
    "\n",
    "The following is the summary of Discourse Relations\n",
    "- Contrasting Relation: Sentence 2 seems to contrast with Sentence 1. The word \"Although\" in Sentence 1 talks about a contrast with the decision pronounced in Sentence 2.\n",
    "- Reason Relation: Sentence 4 seems to give a reason for the decision made in Sentence 3. The word \"because\" specifies that Sentence 4 explains why they travelled to France."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e388c-e670-4628-b318-e55fa331ce1d",
   "metadata": {},
   "source": [
    "#### Entity tracking (code demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5a17f-3044-4f57-b787-f5be55689685",
   "metadata": {},
   "source": [
    "Entity tracking deals with keeping track of the entities (like people, places, or objects) mentioned throughout a text. It's like keeping track of characters throughout a novel. Here's a simple code demo using spaCy for Named Entity Recognition (NER) and tracking those entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02a6f39c-095b-43c7-b45c-2b51170658ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "Sentence: Although she was very busy with office work, Mary felt she had had enough of it. \n",
      "\n",
      "Entity: Mary, Label: PERSON\n",
      "Sentence: She and her spouse decided they needed to go on a holiday. \n",
      "\n",
      "Entity: a holiday, Label: DATE\n",
      "Sentence: They travelled by train to France because they had enough friends in the country.\n",
      "\n",
      "Entity: France, Label: GPE\n",
      "\n",
      "Entity Tracking:\n",
      "Mary: mentioned 1 times\n",
      "a holiday: mentioned 1 times\n",
      "France: mentioned 1 times\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample text.\n",
    "text = \"\"\"\n",
    "Although she was very busy with office work, Mary felt she had had enough of it. \n",
    "She and her spouse decided they needed to go on a holiday. \n",
    "They travelled by train to France because they had enough friends in the country.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Dictionary to track entities\n",
    "entity_tracking = {}\n",
    "\n",
    "# Iterate through the sentences in the doc\n",
    "for sent in doc.sents:\n",
    "    print(f\"Sentence: {sent}\")\n",
    "    # Iterate through named entities in the sentence\n",
    "    for ent in sent.ents:\n",
    "        print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
    "        # Track entities and update if seen again\n",
    "        if ent.text in entity_tracking:\n",
    "            entity_tracking[ent.text] += 1\n",
    "        else:\n",
    "            entity_tracking[ent.text] = 1\n",
    "\n",
    "# Output tracked entities\n",
    "print(\"\\nEntity Tracking:\")\n",
    "for entity, count in entity_tracking.items():\n",
    "    print(f\"{entity}: mentioned {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b1511-5669-41c2-8574-bc8ad6b52655",
   "metadata": {},
   "source": [
    "More professional code for entity tracking is written using packages like spaCy, AllenNLP, or Hugging Face Transformers. Entity code functions first detect, classify, and link entities across texts. These systems make use of NER and Coreference Resolution to track entities even when they are mentioned by pronouns or synonyms. Professional code is frequently a part of larger NLP pipelines that integrate with databases or knowledge graphs to accomplish entity relationships and maintain the required accuracy over long input documents or conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5856a9c-1551-4001-8369-b9fdbbbc8cd8",
   "metadata": {},
   "source": [
    "#### Lexical chains (code demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27353327-796f-4ad0-8e20-9a493ab88c33",
   "metadata": {},
   "source": [
    "A lexical chain is a sequence of related words that are connected either through direct synonyms or semantically related terms. These related words share a common meaning or topic. We discussed lexical chains with an example in the theory section. In this book, we will provide a simple code demo of lexical chains using WordNet from the nltk library. \n",
    "\n",
    "For professional NLP applications, lexical chain code involves advanced algorithms that may be based on WordNet, distributional semantics, or word embeddings like Word2Vec or BERT to pick up semantic relationships between words. Such systems make use of synonyms, hypernyms, and context-based similarities to arrive at accurate lexical chains. This code is often united with text segmentation, word sense disambiguation, and coherence modelling for tasks involving summarization or topic detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3192da5-9853-4a14-895f-cecc2a58022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !python -m nltk.downloader wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569498e5-ccce-4436-bf92-82978e345444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain 1: {'indigo', 'indigotin', 'Indigofera_suffruticosa', 'anil', 'Indigofera_anil'}\n",
      "Chain 2: {'personify', 'represent', 'be', 'equal', 'follow', 'live', 'cost', 'constitute', 'embody', 'comprise', 'make_up', 'exist'}\n",
      "Chain 3: {'type_A', 'adenine', 'ampere', 'a', 'angstrom_unit', 'antiophthalmic_factor', 'vitamin_A', 'angstrom', 'axerophthol', 'amp', 'group_A', 'deoxyadenosine_monophosphate', 'A'}\n",
      "Chain 4: {'debauched', 'lineage', 'line', 'riotous', 'blood', 'blood_line', 'fast', 'degenerate', 'quick', 'bloodline', 'dissolute', 'flying', 'libertine', 'tight', 'fasting', 'firm', 'parentage', 'rake', 'ancestry', 'degraded', 'loyal', 'profligate', 'rip', 'rakehell', 'stock', 'dissipated', 'immobile', 'stemma', 'pedigree', 'descent', 'truehearted', 'line_of_descent', 'roue', 'origin'}\n",
      "Chain 5: {'scholarly_person', 'bookman', 'pupil', 'educatee', 'student', 'scholar'}\n",
      "Chain 6: {'Indiana', 'atomic_number_49', 'In', 'IN', 'inward', 'Hoosier_State', 'inch', 'inwards', 'indium', 'in'}\n",
      "Chain 7: {'execute', 'division', 'incline', 'family', 'outpouring', 'bleed', 'runnel', 'consort', 'running_play', 'bunk', 'pass', 'persist', 'social_class', 'be_given', 'campaign', 'run_for', 'escape', 'hunt_down', 'discharge', 'running_game', 'ply', 'lam', 'guide', 'trial', 'hightail_it', 'classify', 'category', 'go', 'lead', 'head_for_the_hills', 'rivulet', 'sort', 'rill', 'lean', 'carry', 'unravel', 'turn_tail', 'course_of_study', 'melt_down', 'move', 'streak', 'separate', 'foot_race', 'political_campaign', 'ravel', 'fly_the_coop', 'stratum', 'scat', 'race', 'running', 'course_of_instruction', 'melt', 'run_away', 'scarper', 'streamlet', 'function', 'work', 'endure', 'draw', 'hunt', 'test', 'black_market', 'tally', 'operate', 'sort_out', 'track_down', 'play', 'run', 'break_away', 'tend', 'ladder', 'form', 'grade', 'feed', 'die_hard', 'footrace', 'class', 'flow', 'assort', 'extend', 'take_to_the_woods', 'socio-economic_class', 'course', 'year', 'prevail', 'range'}\n",
      "Chain 8: {'atomic_number_2', 'helium', 'He', 'he'}\n",
      "Chain 9: {'very', 'real', 'really', 'selfsame', 'rattling', 'identical'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Sample text\n",
    "text = \"Anil is a blood student in my class. He runs very fast.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Function to find synonyms from WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Build lexical chains\n",
    "lexical_chains = []\n",
    "\n",
    "for word in words:\n",
    "    found_chain = False\n",
    "    word_synonyms = get_synonyms(word)\n",
    "    \n",
    "    # Check if word fits into any existing chain\n",
    "    for chain in lexical_chains:\n",
    "        if chain.intersection(word_synonyms):\n",
    "            chain.update(word_synonyms)\n",
    "            found_chain = True\n",
    "            break\n",
    "    \n",
    "    # If not, start a new chain\n",
    "    if not found_chain and word_synonyms:\n",
    "        lexical_chains.append(set(word_synonyms))\n",
    "\n",
    "# Output lexical chains\n",
    "for i, chain in enumerate(lexical_chains):\n",
    "    print(f\"Chain {i + 1}: {chain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631ad69-d9ec-4fd7-83bc-1f4ce1cf759a",
   "metadata": {},
   "source": [
    "The output characterizes lexical chains as  groups of semantically related words from the input text. Sometimes you see a couple of unrelated terms (not given in the input text) in these lexical chains. This is an example of how lexical chaining can bring in terms that aren't explicitly present but are linked conceptually in the word database. The algorithm sometimes needs fine-tuning to limit the chain generation to the terms that more closely match your input text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57639e-fed8-4689-b510-094d76c45db8",
   "metadata": {},
   "source": [
    "#### Topic modelling (code demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb2657-146f-4785-80a2-84312c1ac567",
   "metadata": {},
   "source": [
    "Topic modelling techniques use word patterns and groupings to identify the key themes or topics in a large collection of texts. In this chapter, we will present a simple demo of topic modelling using Tf-Idf technique. We talked about this technique in detail in the earlier chapters. Professional topic modelling code is often written using advanced techniques like Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF). It leverages Python packages like Gensim or scikit-learn. We will take up LDA in detail in the upcoming chaptes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6de2f5-e8d2-4378-8d65-f7c9d0234e2a",
   "metadata": {},
   "source": [
    "We will follow these five simple steps to write our code. \n",
    "- Prepare Documents: Start with a list of text documents you want to analyse.\n",
    "- Initialize Vectorizer: Create a TfidfVectorizer object. A TfIdf score will convert the input text documents into numerical data. This process ignores stop words.\n",
    "- Transform Text: Use the vectorizer to convert the input text documents into a Tf-Idf matrix. \n",
    "- Get Feature Names: Extract the list of words that were considered in the TF-IDF analysis.\n",
    "- Display Scores: For each document, print the words and their Tf-Idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e93e42-6df3-4e7d-be0d-96045f9e060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1: Although she was very busy with office work, Mary felt she had had enough of it.\n",
      "felt: 0.4472\n",
      "mary: 0.4472\n",
      "work: 0.4472\n",
      "office: 0.4472\n",
      "busy: 0.4472\n",
      "\n",
      "Document 2: She and her spouse decided they needed to go on a holiday.\n",
      "holiday: 0.5000\n",
      "needed: 0.5000\n",
      "decided: 0.5000\n",
      "spouse: 0.5000\n",
      "\n",
      "Document 3: They travelled by train to France because they had enough friends in the country.\n",
      "country: 0.4472\n",
      "friends: 0.4472\n",
      "france: 0.4472\n",
      "train: 0.4472\n",
      "travelled: 0.4472\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text data (documents)\n",
    "\n",
    "documents =   [\"Although she was very busy with office work, Mary felt she had had enough of it.\", \n",
    "                \"She and her spouse decided they needed to go on a holiday.\",\n",
    "                \"They travelled by train to France because they had enough friends in the country.\"]\n",
    "\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the documents into a TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (terms) from the TF-IDF model\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the TF-IDF scores for each document\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {doc_idx + 1}: {doc}\")\n",
    "    # Get the TF-IDF scores for each word in the document\n",
    "    for word_idx in tfidf_matrix[doc_idx].nonzero()[1]:\n",
    "        print(f\"{feature_names[word_idx]}: {tfidf_matrix[doc_idx, word_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde58683-16e6-48c4-a705-3aa30dd94d3f",
   "metadata": {},
   "source": [
    "The output talk about the TF-IDF scores for the words in Document 1, 2, and 3. The higher the score, the more relevant the word is to the content of the document. In the case of document 2, each of these words has a high score of 0.5000. It means all the words are equally important in this document. If a word has a high TF-IDF score, it means that the word is exclusive to that document and it could be a strong pointer of its topic or content. Note that this was a simplified analysis. better results can be obtained using advanced techniques like LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491181d-74c1-4b70-baf0-c202e7811e0d",
   "metadata": {},
   "source": [
    "#### Cohesion analysis (code demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b03d7-7132-40f8-8ad3-166dab80cb2e",
   "metadata": {},
   "source": [
    "Cohesion analysis has two objectives. First, it looks at how well different parts of a text fit together. And second, how they connect to make the text flow smoothly and make sense.\n",
    "In our below demo, we are bringing in a new concept of cosine similarity, which measures the similarity between two vectors. An interpretation of the similarity scores is given below. We will cover this concept in detail in the later pages of this chapter. \n",
    "\n",
    "We have used a basic TF-Idf technique in our code demo. More advanced vectorization techniques used by professionals for cohesion insight include word embeddings like Word2Vec and GloVe to capture more nuanced insights into text similarity and cohesion. We will take up these techniques later in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee496f73-c4ad-43a6-b462-bdd146c77f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "Similarity between Sentence 1 and Sentence 1: 1.00\n",
      "Similarity between Sentence 1 and Sentence 2: 0.32\n",
      "Similarity between Sentence 1 and Sentence 3: 0.19\n",
      "Similarity between Sentence 2 and Sentence 1: 0.32\n",
      "Similarity between Sentence 2 and Sentence 2: 1.00\n",
      "Similarity between Sentence 2 and Sentence 3: 0.10\n",
      "Similarity between Sentence 3 and Sentence 1: 0.19\n",
      "Similarity between Sentence 3 and Sentence 2: 0.10\n",
      "Similarity between Sentence 3 and Sentence 3: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Revised sentences\n",
    "sentences = [\n",
    "    \"Mary is feeling overwhelmed with her busy office job but finds some relief in her evening walks.\",\n",
    "    \"Mary feels overwhelmed with her hectic work schedule, yet she finds relaxation in her daily evening walks.\",\n",
    "    \"Despite a busy workday, Mary enjoys unwinding with a walk in the evening.\"\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the sentences into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        print(f\"Similarity between Sentence {i + 1} and Sentence {j + 1}: {cosine_sim_matrix[i, j]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fb3ec-d0f7-4e5f-9556-561cbf855f92",
   "metadata": {},
   "source": [
    "Interpretation of the output:\n",
    "- Interpreting the scores: Higher scores specify more cohesive sentences with shared vocabulary and similar content.\n",
    "- Low Inter-Sentence Similarity: The highest is 0.32. It shows limited shared content or vocabulary.\n",
    "- Minimal Overlap: Low similarity scores of 0.10 and 0.19 indicate minimal thematic or lexical overlap between the sentences.\n",
    "- A similarity score of 1.00 indicates perfect similarity with itself.\n",
    "Similarity scores of 0 indicate no shared vocabulary and similarity of content between sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30c26b-9ac6-46f4-9489-8bdb40e152aa",
   "metadata": {},
   "source": [
    "#### Temporal relation identification (code demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb400bf2-d855-41ab-a7c6-b18428ca9453",
   "metadata": {},
   "source": [
    "Temporal relation identification is the process of finding time-based relationships between events in an input text. For instance, take the sentence \"I want to complete this chapter before lunch;\" the word \" before\" indicates the time-based connection between two events. It talks about finishing work earlier than lunch.\n",
    "\n",
    "Our code demo for temporal relation identification would first perform dependency parsing for extracting events and detect temporal signals, which are swords like \"before,\" \"after,\" \"during,\" \"until,\" and \"while.\". After this, the classification of events will be done through rule-based methods or machine-learning models. This approach aims to find the sequence and timing of actions in a given input text. We will utilise spaCy to extract events. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2f521-e822-4588-ae53-d755e3df460a",
   "metadata": {},
   "source": [
    "Our program below follows these five steps\n",
    "- Import Libraries and Load Model\n",
    "- Feature Extraction from Sentences\n",
    "- Prepare Training Data\n",
    "- Train Random Forest Classifier\n",
    "- Predict Temporal Relations in New Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0884e7fe-b35d-46ce-858b-000793561116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: John started his project before the deadline.\n",
      "Detected Temporal Signal: before, Prediction: Temporal relation\n",
      "\n",
      "Sentence: She arrived after the party had begun.\n",
      "Detected Temporal Signal: after, Prediction: Temporal relation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import spacy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the spaCy model for dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample sentences with temporal relations\n",
    "sentences = [\n",
    "    \"She finished her work before going to lunch.\",\n",
    "    \"He went to the gym after work.\",\n",
    "    \"They waited until the show started.\",\n",
    "    \"The meeting was delayed during the storm.\"\n",
    "]\n",
    "\n",
    "# Temporal signals\n",
    "temporal_signals = [\"before\", \"after\", \"during\", \"until\", \"while\"]\n",
    "\n",
    "# Feature extraction - identifying events and temporal signals\n",
    "def extract_features(sent):\n",
    "    doc = nlp(sent)\n",
    "    events = []\n",
    "    temporal_relation = \"\"\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\":  # Event (verb) extraction\n",
    "            events.append(token.lemma_)\n",
    "        if token.text in temporal_signals:  # Temporal signal detection\n",
    "            temporal_relation = token.text\n",
    "    return events, temporal_relation\n",
    "\n",
    "# Prepare training data - sentences, events, and labels (1 for temporal relation present, 0 otherwise)\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    events, signal = extract_features(sentence)\n",
    "    if signal:  # If temporal signal is present, we classify as 1\n",
    "        X.append([len(events)])  # Simple feature: number of events\n",
    "        y.append(1)\n",
    "    else:\n",
    "        X.append([0])\n",
    "        y.append(0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict on new sentences\n",
    "new_sentences = [\"John started his project before the deadline.\",\n",
    "                 \"She arrived after the party had begun.\"]\n",
    "\n",
    "for new_sent in new_sentences:\n",
    "    events, signal = extract_features(new_sent)\n",
    "    prediction = clf.predict([[len(events)]])\n",
    "    print(f\"Sentence: {new_sent}\")\n",
    "    print(f\"Detected Temporal Signal: {signal}, Prediction: {'Temporal relation' if prediction == 1 else 'No temporal relation'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f975f-c03b-47de-8c7e-61fcfb332679",
   "metadata": {},
   "source": [
    "We will throw some line on the function def extract_features(sent): for better understanding.\n",
    "- extract_features(sent): Processes each sentence using spaCy. The function returns the list of events (verbs) and the temporal relation signal (if present).\n",
    "- doc = nlp(sent): Prepared the sentence and gets it into a structured format for analysis.\n",
    "- for token in doc:: Loops through each word in the sentence.\n",
    "- token.dep_ == \"ROOT\": Checks if the word is the main verb (the root action) and stores it.\n",
    "- if token.text in temporal_signals:: If a word is found in the list of temporal signals, it is stored that as a temporal signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf5716-feeb-4c01-9d3a-a2b3eabf04e0",
   "metadata": {},
   "source": [
    "Converting words into ML model format: Without vectorizing the entire sentence, the logic uses a simple logic for converting words into numbers. It counts the number of verbs (events) as features in X, and puts a binary label (1 or 0) for y based on the presence of temporal signals. This keeps the dataset simple and in numerical format without using  a vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78481e1f-7744-4899-b9bc-28368a5b943b",
   "metadata": {},
   "source": [
    "The output confirms that the code is effectively detecting temporal signals present in the input sentences. The model is also correctly predicting the existence of temporal relations based on these signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00c768-5a39-4f0c-be08-c5af4f396639",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> Code Snippet 7.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
