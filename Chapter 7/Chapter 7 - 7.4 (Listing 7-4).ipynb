{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080f80a-9939-4f34-b2da-ed67ed0d038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running a shallow neural network program to demonstrate skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f23890-3380-4bac-bed2-f48095eef123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.0738  \n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0708\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0690\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0673   \n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0649\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0628\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0604\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0604 \n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0559\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0550  \n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0521\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0502\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0483\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0457  \n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0449\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0428 \n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0422\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0406\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - loss: 2.0365\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0389\n",
      "Embedding for 'great': [ 0.01801779 -0.0028935  -0.04373218  0.06102643 -0.04138939 -0.05270927\n",
      "  0.1154007  -0.08093932 -0.07753371 -0.02089559]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# For numerical operations\n",
    "import numpy as np \n",
    "# For deep learning models\n",
    "import tensorflow as tf  \n",
    "# For creating a linear model\n",
    "from tensorflow.keras.models import Sequential \n",
    "# Layers for model building\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten \n",
    "# For tokenizing text\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "# For generating Skip-gram pairs\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams  \n",
    "\n",
    "# Sample corpus for demonstration\n",
    "sentences = [\"he is a great scholar\", \"a great scholar writes great papers\"]  \n",
    "\n",
    "# Initialize tokenizer and fit on text\n",
    "tokenizer = Tokenizer()  \n",
    "# Fit tokenizer on sample sentences\n",
    "tokenizer.fit_on_texts(sentences)  \n",
    "# Get vocabulary size\n",
    "total_words = len(tokenizer.word_index) + 1  \n",
    "\n",
    "# Convert sentences to sequences of word indices\n",
    "# Convert sentences to integer sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)  \n",
    "\n",
    "# Generate word pairs (center, context) for Skip-gram\n",
    "# Initialize list to store Skip-gram pairs\n",
    "skip_gram_pairs = []  \n",
    "for seq in sequences:  \n",
    "    # For each sentence sequence\n",
    "    pairs, _ = skipgrams(seq, vocabulary_size=total_words, window_size=2)  \n",
    "    # Generate Skip-gram pairs\n",
    "    skip_gram_pairs.extend(pairs)  # Add pairs to list\n",
    "\n",
    "# Separate input (center) and output (context) words\n",
    "input_words, context_words = zip(*skip_gram_pairs)  \n",
    "# Convert input words to array\n",
    "input_words = np.array(input_words)  \n",
    "# Convert context words to array\n",
    "context_words = np.array(context_words)  \n",
    "\n",
    "# Define Skip-gram model\n",
    "model = Sequential() \n",
    "# Embedding layer with vector size 10\n",
    "model.add(Embedding(total_words, 10, input_length=1))  \n",
    "# Flatten the output\n",
    "model.add(Flatten())  \n",
    "# Output layer for vocabulary-size classification\n",
    "model.add(Dense(total_words, activation='softmax'))  \n",
    "\n",
    "# Compile model with categorical cross-entropy\n",
    "# Set optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')  \n",
    "\n",
    "# Train model on (input, context) pairs\n",
    "model.fit(input_words, context_words, epochs=20, verbose=1)  \n",
    "\n",
    "# Get embeddings for a specific word, e.g., \"great\"\n",
    "# Find index of word \"great\"\n",
    "great_index = tokenizer.word_index['great']  \n",
    "# Get embedding vector for \"great\"\n",
    "great_embedding = model.layers[0].get_weights()[0][great_index]  \n",
    "# Print embedding\n",
    "print(\"Embedding for 'great':\", great_embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f140a76-b57a-4cb6-9f3b-a7aff006f389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
