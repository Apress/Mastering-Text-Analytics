{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080f80a-9939-4f34-b2da-ed67ed0d038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running a shallow neural network program to demonstrate CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4704976b-1fcd-4fcd-9721-ceca8b1cd9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shailendra Kadre\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step - accuracy: 0.1667 - loss: 2.6119\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3333 - loss: 2.6053\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5000 - loss: 2.5986\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 2.5919\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5000 - loss: 2.5852\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5000 - loss: 2.5785\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6667 - loss: 2.5718\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6667 - loss: 2.5651\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8333 - loss: 2.5583\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 2.5515\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.5447\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.5378\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 2.5310\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 2.5240\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.5171\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 2.5100\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 2.5030\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.4958\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 2.4887\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.4814\n",
      "Embedding for 'great': [ 0.019222   -0.01932465 -0.03971442 -0.02196687 -0.02987816  0.00716395\n",
      " -0.01269478  0.02227857  0.01485946 -0.07757436]\n"
     ]
    }
   ],
   "source": [
    "# Ignore deprecation warnings for cleaner output.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # Sequential model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten  # Layers for neural network\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Tokenizer for text preprocessing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For padding sequences\n",
    "\n",
    "# Sample corpus of sentences (training curpus) \n",
    "sentences = [\n",
    "    \"he is a great scholar\",\n",
    "    \"a great scholar writes great papers\",\n",
    "    \"the scholar is very great\",\n",
    "    \"great ideas come from great minds\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)  # Fit tokenizer on sentences\n",
    "total_words = len(tokenizer.word_index) + 1  # Total unique words in corpus\n",
    "\n",
    "# Initialize lists for CBOW input-output pairs\n",
    "input_data = []  # Context words\n",
    "output_data = []  # Target word\n",
    "\n",
    "# Define the context window size\n",
    "window_size = 2\n",
    "\n",
    "# Create input-output pairs for CBOW\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    for i in range(window_size, len(words) - window_size):  # Loop through words\n",
    "        context = []\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j != i:  # Skip target word\n",
    "                context.append(tokenizer.word_index[words[j]])  # Append context word index\n",
    "        input_data.append(context)  # Add context to input data\n",
    "        output_data.append(tokenizer.word_index[words[i]])  # Add target word to output data\n",
    "\n",
    "# Pad input sequences to ensure uniform length\n",
    "input_data = pad_sequences(input_data, padding='post')\n",
    "\n",
    "# One-hot encode output data for categorical labels\n",
    "output_data = np.array(output_data)\n",
    "output_data = np.eye(total_words)[output_data]  # Convert to one-hot\n",
    "\n",
    "# Build CBOW model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=10, input_length=window_size * 2))  # Embedding layer\n",
    "model.add(Flatten())  # Flatten to 1D\n",
    "model.add(Dense(total_words, activation='softmax'))  # Output layer with softmax for word prediction\n",
    "\n",
    "# Compile model with optimizer, loss, and metrics\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on input-output pairs\n",
    "model.fit(input_data, output_data, epochs=20, verbose=1)\n",
    "\n",
    "# Retrieve embedding for the word \"great\"\n",
    "great_index = tokenizer.word_index['great']  # Index of \"great\" in vocabulary\n",
    "great_embedding = model.layers[0].get_weights()[0][great_index]  # Extract embedding\n",
    "print(\"Embedding for 'great':\", great_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
