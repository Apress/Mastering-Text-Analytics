{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b09633",
   "metadata": {},
   "source": [
    "#### Tutorial on Term-Document Matrix, TF-IDF, Chunking, Named Entity Recognition (NER), and Word Sense Disambiguation (WSD) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these libraries, if not done already. \n",
    "# !pip install nltk spacy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1addab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a6d9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b2daed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample Documents.\n",
    "documents = [\n",
    "    \"Apple and banana are fruits.\",\n",
    "    \"I like to eat apple pie.\",\n",
    "    \"The banana pie is delicious.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86002942",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1264e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_example(doc):\n",
    "    chunks = []\n",
    "    for sent in doc.sents:\n",
    "        for chunk in sent.noun_chunks:\n",
    "            chunks.append(chunk.text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7b5bb",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1880028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_example(doc):\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef39e8d",
   "metadata": {},
   "source": [
    "#### Word Sense Disambiguation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e12790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd_example(word, context):\n",
    "    # Simplistic approach for demonstration\n",
    "    if word == \"apple\":\n",
    "        if \"pie\" in context:\n",
    "            return \"The tech company\"\n",
    "        else:\n",
    "            return \"The fruit\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104f38d",
   "metadata": {},
   "source": [
    "#### Term-Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c23ea342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-Document Matrix:\n",
      "    and  apple  are  banana  delicious  eat  fruits  is  like  pie  the  to\n",
      "0    1      1    1       1          0    0       1   0     0    0    0   0\n",
      "1    0      1    0       0          0    1       0   0     1    1    0   1\n",
      "2    0      0    0       1          1    0       0   1     0    1    1   0\n"
     ]
    }
   ],
   "source": [
    "# Create the Term-Document Matrix using raw counts.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df_term_doc = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Term-Document Matrix:\\n\", df_term_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344754d",
   "metadata": {},
   "source": [
    "#### Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4b47c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "         and     apple       are    banana  delicious       eat    fruits  \\\n",
      "0  0.490479  0.373022  0.490479  0.373022   0.000000  0.000000  0.490479   \n",
      "1  0.000000  0.373022  0.000000  0.000000   0.000000  0.490479  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.373022   0.490479  0.000000  0.000000   \n",
      "\n",
      "         is      like       pie       the        to  \n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.490479  0.373022  0.000000  0.490479  \n",
      "2  0.490479  0.000000  0.373022  0.490479  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example Documents\n",
    "documents = [\n",
    "    \"Apple and banana are fruits.\",\n",
    "    \"I like to eat apple pie.\",\n",
    "    \"The banana pie is delicious.\"\n",
    "]\n",
    "\n",
    "# Create the TF-IDF Matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF Matrix:\\n\", df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a097a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apple and banana are fruits., I like to eat apple pie., The banana pie is delicious.]\n"
     ]
    }
   ],
   "source": [
    "# Process sample documents.\n",
    "docs_spacy = [nlp(doc) for doc in documents]\n",
    "print(docs_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcd3d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: [['Apple', 'banana', 'fruits'], ['I', 'apple pie'], ['The banana pie']]\n"
     ]
    }
   ],
   "source": [
    "# Try chunking.\n",
    "# Uses SpaCy to extract noun chunks from each document.\n",
    "chunks = [chunking_example(doc) for doc in docs_spacy]\n",
    "print(\"Chunks:\", chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "921f9e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [[('Apple', 'ORG')], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Try NER.\n",
    "# Identifies named entities such as persons, locations, and organizations in the documents.\n",
    "ner_results = [ner_example(doc) for doc in docs_spacy]\n",
    "print(\"Named Entities:\",ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d317339",
   "metadata": {},
   "source": [
    "- No named entities detected in documents 2 and 3. It is likely because \"apple pie\" and “banana pie” are not recognized as a named entity.\n",
    "- It could be because of model limitations of the pre-trained SpaCy model (en_core_web_sm). Larger models (en_core_web_md or en_core_web_lg), have more extensive training data. So, you can try with them. By using a larger model or adjusting the text, you may get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94b5fa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSD Results: ['The fruit', 'The tech company', 'The tech company']\n"
     ]
    }
   ],
   "source": [
    "# Try WSD.\n",
    "'''\n",
    "A simplistic approach is used here to demonstrate \n",
    "how \"apple\" might be interpreted based on context.\n",
    "'''\n",
    "\n",
    "wsd_results = [wsd_example(\"apple\", doc.text) for doc in docs_spacy]\n",
    "print(\"WSD Results:\", wsd_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
