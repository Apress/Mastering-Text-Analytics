{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2327021b",
   "metadata": {},
   "source": [
    "### Code Demos for Multiple Lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ba80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb44304",
   "metadata": {},
   "source": [
    "#### WordNet lemmatizer (NLTK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf22683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a6eec28",
   "metadata": {},
   "source": [
    "The SpaCy lemmatizer is designed using a combination of rule-based and statistical methods to find the base forms of words. It comes integrated with the SpaCy NLP library. This lemmatizer is predominantly effective for high performance and context-aware lemmatization because of its accuracy in reducing words to their base forms. Named entity recognition (NER), dependency parsing, advanced text analytics, and large-scale data processing are the NLP applications that especially leverage this lemmatizer. Below is it's code demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spacy if you have not done it already.\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SpaCy English model.\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d76f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OUTPUT\n",
      "\n",
      "\n",
      "Original Text:  The SpaCy lemmatizer is designed using a combination of rule-based and statistical methods \n",
      "to find the base forms of words. It comes integrated with the SpaCy NLP library.\n",
      "\n",
      "Lemmatized Tokens:  [('The', 'the'), ('SpaCy', 'SpaCy'), ('lemmatizer', 'lemmatizer'), ('is', 'be'), ('designed', 'design'), ('using', 'use'), ('a', 'a'), ('combination', 'combination'), ('of', 'of'), ('rule', 'rule'), ('-', '-'), ('based', 'base'), ('and', 'and'), ('statistical', 'statistical'), ('methods', 'method'), ('\\n', '\\n'), ('to', 'to'), ('find', 'find'), ('the', 'the'), ('base', 'base'), ('forms', 'form'), ('of', 'of'), ('words', 'word'), ('.', '.'), ('It', 'it'), ('comes', 'come'), ('integrated', 'integrate'), ('with', 'with'), ('the', 'the'), ('SpaCy', 'SpaCy'), ('NLP', 'NLP'), ('library', 'library'), ('.', '.'), ('\\n', '\\n')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy English model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create Sample text.\n",
    "text = \"\"\"The SpaCy lemmatizer is designed using a combination of rule-based and statistical methods \n",
    "to find the base forms of words. It comes integrated with the SpaCy NLP library.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text using SpaCyEnglish model.\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get lemmatized forms.\n",
    "lemmatized_tokens = [(token.text, token.lemma_) for token in doc]\n",
    "\n",
    "# Print the output.\n",
    "print(\"\\n\")\n",
    "print(\"OUTPUT\")\n",
    "print(\"\\n\")\n",
    "print(\"Original Text: \", text)\n",
    "print(\"Lemmatized Tokens: \", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc760c7c",
   "metadata": {},
   "source": [
    "- Note that all the base forms extracted with SpaCy are accurate and carry usable meanings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36848824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef32e127",
   "metadata": {},
   "source": [
    "#### SpaCy lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd766741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91851457",
   "metadata": {},
   "source": [
    "SpaCy lemmetizer is known for its high speed and efficiency. It is optimized to swiftly process large amounts of text data. NLTK also stands for a solid performance, but tends to be slower as compared to  spaCy when processing huge of text data. Below is it's code demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017dec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule\n",
      "[' ', 'the', 'SpaCy', 'lemmatizer', 'be', 'design', 'use', 'a', 'combination', 'of', 'rule', '-', 'base', 'and', 'statistical', 'method', '\\n                     ', 'to', 'find', 'the', 'base', 'form', 'of', 'word', '.', 'it', 'come', 'integrate', 'with', 'the', 'SpaCy', 'NLP', 'library', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# These English pipelines have an inbuilt rule-based lemmatizer.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "print(lemmatizer.mode)  # 'rule'\n",
    "\n",
    "sample_doc = nlp(\"\"\" The SpaCy lemmatizer is designed using a combination of rule-based and statistical methods \n",
    "                     to find the base forms of words. It comes integrated with the SpaCy NLP library.\"\"\")\n",
    "print([token.lemma_ for token in sample_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0b34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96cc7fdc",
   "metadata": {},
   "source": [
    "#### TextBlob lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de481df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4ee5f4f",
   "metadata": {},
   "source": [
    "TextBlob performs faster when compared to nltk. It can be easily deployed with lesser computing resources. TextBlob is simpler to use and it supports many functions that are not available in nltk. Below is its code demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install TextBlob.\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2541184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample words: ['TextBlob', 'performs', 'faster', 'when', 'compared', 'to', 'nltk', 'It', 'can', 'be', 'easily', 'deployed', 'with', 'lesser', 'computing', 'resources', 'TextBlob', 'is', 'simpler', 'to', 'use', 'and', 'it', 'supports', 'many', 'functions', 'that', 'are', 'not', 'available', 'in', 'nltk']\n",
      "Lemmatized words: ['TextBlob', 'performs', 'faster', 'when', 'compared', 'to', 'nltk', 'It', 'can', 'be', 'easily', 'deployed', 'with', 'lesser', 'computing', 'resource', 'TextBlob', 'is', 'simpler', 'to', 'use', 'and', 'it', 'support', 'many', 'function', 'that', 'are', 'not', 'available', 'in', 'nltk']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "# Create sample text.\n",
    "text = \"\"\" TextBlob performs faster when compared to nltk. It can be easily deployed with lesser computing resources. \n",
    "          TextBlob is simpler to use and it supports many functions that are not available in nltk. \n",
    "\n",
    "       \"\"\"\n",
    "\n",
    "# Create a TextBlob object.\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Tokenize the sample text.\n",
    "words = blob.words\n",
    "\n",
    "# Lemmatize.\n",
    "lemmatized_words = [Word(word).lemmatize() for word in words]\n",
    "\n",
    "# Print the output.\n",
    "print(\"Original sample words:\", words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6667a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d6c29d",
   "metadata": {},
   "source": [
    "Code Snippet 5.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
