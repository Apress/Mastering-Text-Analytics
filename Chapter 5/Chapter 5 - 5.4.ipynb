{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2327021b",
   "metadata": {},
   "source": [
    "### Chapter 5 Code Demo for NLP Data Pre-processing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7ec9f",
   "metadata": {},
   "source": [
    "#### YouTube Comments Spam Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a390ea3",
   "metadata": {},
   "source": [
    "We solved the same problem in the Chapter 1 code demo. This time we will attempt solve it here again with some data-pre-processing steps and see if we get any improvements in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5346a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# The below code is for working with machine learning model.\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec0ef049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings.\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5976dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9fc85ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code is for working with machine learning model.\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d488c768",
   "metadata": {},
   "source": [
    "#### A quick look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3dbd2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data files [1] available in the same folder as this code.\n",
    "Youtube01_psy = pd.read_csv('Youtube01-Psy.csv')\n",
    "Youtube02_katyperry = pd.read_csv('Youtube02-KatyPerry.csv')\n",
    "Youtube03_lmfao = pd.read_csv('Youtube03-LMFAO.csv')\n",
    "Youtube04_eminem = pd.read_csv('Youtube04-Eminem.csv')\n",
    "Youtube05_shakira = pd.read_csv('Youtube05-Shakira.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6415943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACombine all five datasets.\n",
    "combined_df = pd.concat([Youtube01_psy, Youtube02_katyperry, Youtube03_lmfao, Youtube04_eminem, Youtube05_shakira])\n",
    "\n",
    "# Reset the index\n",
    "combined_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d38e0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30d1df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the useful \"CONTENT\" and \"CLASS\" columns.\n",
    "combined_df = combined_df[[\"CONTENT\", \"CLASS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4cabf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                CONTENT  CLASS\n",
      "1127  The best Song i saw ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏èüòçüòçüòçüòçüòçüòçüòçüòòüòòüòò...      0\n",
      "599   Hey Katycats! We are releasing a movie at midn...      1\n",
      "574   want to win borderlands the pre-sequel? check ...      1\n",
      "533   Awesome video this is one of my favorite  song...      0\n",
      "747             Love this video and the song of courseÔªø      0\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 rows\n",
    "random_sample = combined_df.sample(n=5)\n",
    "print(random_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d0796",
   "metadata": {},
   "source": [
    "- Note the special characters and misalignment doe to spaces in CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c07bb55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                CONTENT  CLASS\n",
      "532   http://www.googleadservices.com/pagead/aclk?sa...      1\n",
      "1447            I love this song sooooooooooooooo muchÔªø      0\n",
      "1901  Hey youtubers... I really appreciate all of yo...      1\n",
      "268   https://www.facebook.com/pages/Mathster-WP/149...      1\n",
      "1304  sorry but eminmem is a worthless wife beating ...      0\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 rows\n",
    "random_sample = combined_df.sample(n=5)\n",
    "print(random_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc072ff1",
   "metadata": {},
   "source": [
    "#### Convert the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67a041b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all comments to string type for further processing.\n",
    "combined_df[\"CONTENT\"] = combined_df[\"CONTENT\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6236994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               CONTENT  CLASS  \\\n",
      "404  YAY IM THE 11TH COMMENTER!!!!!                ...      1   \n",
      "907               Check out this playlist on YouTube:Ôªø      1   \n",
      "942  View 851.247.920<br /><br />¬†Best youtube Vide...      1   \n",
      "\n",
      "                                       text_lower_case  \n",
      "404  yay im the 11th commenter!!!!!                ...  \n",
      "907               check out this playlist on youtube:Ôªø  \n",
      "942  view 851.247.920<br /><br />¬†best youtube vide...  \n"
     ]
    }
   ],
   "source": [
    "# Convert everything in to lower case.\n",
    "combined_df[\"text_lower_case\"] = combined_df[\"CONTENT\"].str.lower()\n",
    "# Randomly select 3 rows.\n",
    "random_sample_lower = combined_df.sample(n=3)\n",
    "print(random_sample_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "701d1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ombined_df[\"CONTENT\"] as we will work only with combined_df[\"text_lower_case\"].\n",
    "# The punctuations present are - !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n",
    "# combined_df = combined_df.drop(columns=[\"CONTENT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3c92d560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTENT', 'CLASS', 'text_lower_case'], dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482f36e",
   "metadata": {},
   "source": [
    "#### Remove all unwanted punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76ec6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation to remove\n",
    "punctuation_to_remove = \"!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\"\n",
    "\n",
    "# Remove the above punctuations from the # Remove the above punctuations from the \"text_lower_case\" column.\n",
    "combined_df[\"text_lower_case\"] = combined_df[\"text_lower_case\"].str.translate(str.maketrans('', '', punctuation_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9cc7859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                CONTENT  CLASS  \\\n",
      "173                     http://www.gofundme.com/gvr7xgÔªø      1   \n",
      "277   Hey, join me on ts≈´, a publishing platform whe...      1   \n",
      "1828                          Shakira is very beautiful      0   \n",
      "25    marketglory . com/strategygame/andrijamatf ear...      1   \n",
      "580   Thank you KatyPerryVevo for your instagram lik...      1   \n",
      "1240  all u should go check out j rants vi about eminem      1   \n",
      "1788              Please visit this Website: oldchat.tk      1   \n",
      "962    <br />Please help me get 100 subscribers by t...      1   \n",
      "1857                                            Love it      0   \n",
      "301   http://hackfbaccountlive.com/?ref=4436607  psy...      1   \n",
      "\n",
      "                                        text_lower_case  \n",
      "173                           httpwwwgofundmecomgvr7xgÔªø  \n",
      "277   hey join me on ts≈´ a publishing platform where...  \n",
      "1828                          shakira is very beautiful  \n",
      "25    marketglory  comstrategygameandrijamatf earn r...  \n",
      "580   thank you katyperryvevo for your instagram lik...  \n",
      "1240  all u should go check out j rants vi about eminem  \n",
      "1788                please visit this website oldchattk  \n",
      "962    br please help me get 100 subscribers by the ...  \n",
      "1857                                            love it  \n",
      "301   httphackfbaccountlivecomref4436607  psy news o...  \n"
     ]
    }
   ],
   "source": [
    "# Randomly select 10 rows.\n",
    "random_sample_lower = combined_df.sample(n=10)\n",
    "print(random_sample_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e58b37",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "feba32c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shailendra\n",
      "[nltk_data]     Kadre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a78f5528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTENT', 'CLASS', 'text_lower_case'], dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d905d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from the \"text_lower_case\" column\n",
    "combined_df[\"text_lower_case\"] = combined_df[\"text_lower_case\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9ab54e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                CONTENT  CLASS  \\\n",
      "1025  <a href=\"https://m.freemyapps.com/share/url/10...      1   \n",
      "181                         Please check out my vidiosÔªø      1   \n",
      "724   This awesome song needed 4 years to reach to 8...      0   \n",
      "1296  5 years and i still dont get the music video h...      0   \n",
      "878                                                omgÔªø      0   \n",
      "\n",
      "                                        text_lower_case  \n",
      "1025  hrefhttpsmfreemyappscomshareurl10b35481httpsmf...  \n",
      "181                                please check vidiosÔªø  \n",
      "724   awesome song needed 4 years reach 800 mil view...  \n",
      "1296   5 years still dont get music video help someoneÔªø  \n",
      "878                                                omgÔªø  \n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 rows.\n",
    "random_sample_lower = combined_df.sample(n=5)\n",
    "print(random_sample_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b66bd",
   "metadata": {},
   "source": [
    "The frequent or rare words removal is based on the specific goals of your NLP task and the nature of dataset in hand. Frequent words, removed because of their minimal informational value. Removing frequent and rare words helps to reduce noise in the data and allows focus on more meaningful words. It improves the performance of several NLP algorithms as they can now focus on content-rich words that contributes to the overall analysis. Rare words are particularly removed as they have limited relevance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5c865",
   "metadata": {},
   "source": [
    "- For this problem, we will not remove the frequent and rare words.\n",
    "- Below we will just share the code on how to remove them, if your project needs it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fe1b6",
   "metadata": {},
   "source": [
    "#### Remove frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f70c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent words with their frequencies:\n",
      "check: 559\n",
      "video: 294\n",
      "Ôªø: 267\n",
      "like: 235\n",
      "please: 231\n",
      "song: 231\n",
      "subscribe: 209\n",
      "love: 189\n",
      "channel: 173\n",
      "music: 144\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Combine all text into a single string and split into individual words.\n",
    "all_words = ' '.join(combined_df[\"text_lower_case\"]).split()\n",
    "\n",
    "# Count the frequency of every word.\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Determine the threshold for frequent words (top 10 most common words).\n",
    "most_common_words = word_counts.most_common(10)\n",
    "\n",
    "# Print the frequent words with their frequencies.\n",
    "print(\"Frequent words with their frequencies:\")\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Set of the frequent words.\n",
    "frequent_words = {word for word, count in most_common_words}\n",
    "\n",
    "# Remove frequent words from the combined_df[\"text_lower_case\"].\n",
    "combined_df[\"frequent_removed\"] = combined_df[\"text_lower_case\"].apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word not in frequent_words])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6fdb63ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTENT', 'CLASS', 'text_lower_case', 'frequent_removed'], dtype='object')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8eaadf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                   frequent_removed\n",
      "1939      1  peoples earth seen perform every form evil lei...\n",
      "459       0  comment randomly get lots likes replies reason...\n",
      "896       0                       almost 1 billion views niceÔªø\n",
      "1799      0                                      she39s pretty\n",
      "1426      0                charlieee dddd saw lost understandÔªø\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 rows.\n",
    "random_sample_lower = combined_df[['CLASS', 'frequent_removed']].sample(n=5)\n",
    "print(random_sample_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398eaa64",
   "metadata": {},
   "source": [
    "#### Remove rare words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b5031a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rare words with their frequencies:\n",
      "anyway: 1\n",
      "kobyoshi02: 1\n",
      "monkeys: 1\n",
      "shirtplease: 1\n",
      "test: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Combine all text into a single string and split into individual words.\n",
    "all_words = ' '.join(combined_df[\"text_lower_case\"]).split()\n",
    "\n",
    "# Count the frequency of every word.\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Define the threshold for rare words.\n",
    "threshold = 5\n",
    "rare_words = {word: count for word, count in word_counts.items() if count < threshold}\n",
    "\n",
    "# Sort the rare words by frequency in ascending order and keep the top 5.\n",
    "sorted_rare_words = sorted(rare_words.items(), key=lambda item: item[1])\n",
    "top_5_rare_words = sorted_rare_words[:5]\n",
    "\n",
    "# Print the top 5 rare words with their frequencies.\n",
    "print(\"Top 5 rare words with their frequencies:\")\n",
    "for word, count in top_5_rare_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Remove rare words from combined_df[\"text_lower_case\"].\n",
    "combined_df[\"rare_removed\"] = combined_df[\"text_lower_case\"].apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word not in rare_words])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6d04b",
   "metadata": {},
   "source": [
    "- All the removed rare words are not displayed here as the list is very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c5058d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CLASS', 'text_lower_case', 'frequent_removed', 'rare_removed'], dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8ed7990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                      lower_removed\n",
      "1455      0                                                 so\n",
      "1787      1                          please visit this website\n",
      "35        0  why is a korean song so big in the does that m...\n",
      "1622      1                   check out this video on youtubeÔªø\n",
      "1386      0                                                  Ôªø\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 rows.\n",
    "random_sample_lower = combined_df[['CLASS', 'lower_removed']].sample(n=5)\n",
    "print(random_sample_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949940ee",
   "metadata": {},
   "source": [
    " - Below we will apply Stemming and Lemmatisation directly to combined_df[\"text_lower_case\"] as they are necessary steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682c69f",
   "metadata": {},
   "source": [
    "#### Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1bc78083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Shailendra\n",
      "[nltk_data]     Kadre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure that the necessary NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to stem words in a text and return the stemmed version\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b2e649be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to tcombined_df[\"text_lower_case\"].\n",
    "combined_df[\"text_lower_case\"] = combined_df[\"text_lower_case\"].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65f681b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all stemmed text into a single string and split into individual words\n",
    "all_stemmed_words = ' '.join(combined_df[\"text_lower_case\"]).split()\n",
    "\n",
    "# Count the frequency of every stemmed word\n",
    "word_counts = Counter(all_stemmed_words)\n",
    "\n",
    "# Get the top 5 stemmed words and their frequencies\n",
    "top_5_stemmed_words = word_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "759b7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from stemmed words to their original base forms\n",
    "base_form_mapping = {}\n",
    "for text in combined_df[\"text_lower_case\"]:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        if stemmed_word not in base_form_mapping:\n",
    "            base_form_mapping[stemmed_word] = set()\n",
    "        base_form_mapping[stemmed_word].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "372a70b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 stemmed words with their original base forms:\n",
      "Stemmed Word: check, Count: 568, Original Words: check\n",
      "Stemmed Word: video, Count: 361, Original Words: video\n",
      "Stemmed Word: song, Count: 274, Original Words: song\n",
      "Stemmed Word: Ôªø, Count: 267, Original Words: Ôªø\n",
      "Stemmed Word: like, Count: 256, Original Words: like\n"
     ]
    }
   ],
   "source": [
    "# Display the top 5 stemmed words with their original base forms\n",
    "print(\"Top 5 stemmed words with their original base forms:\")\n",
    "for stemmed_word, count in top_5_stemmed_words:\n",
    "    base_forms = base_form_mapping.get(stemmed_word, [])\n",
    "    base_form_display = ', '.join(base_forms)  # Display unique base forms\n",
    "    print(f\"Stemmed Word: {stemmed_word}, Count: {count}, Original Words: {base_form_display}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87b8a4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTENT', 'CLASS', 'text_lower_case', 'frequent_removed',\n",
       "       'rare_removed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a393a22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>text_lower_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>huh anyway check youtub channel kobyoshi02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hey guy check new channel first vid us monkey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>test say murdevcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>shake sexi ass channel enjoy Ôªø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>watchvvtarggvgtwq check Ôªø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>hey check new websit site kid stuff kidsmediau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>subscrib channel Ôªø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>turn mute soon came want check viewsÔªø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>check channel funni videosÔªø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>u shouldd check channel tell nextÔªø</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CLASS                                    text_lower_case\n",
       "0      1         huh anyway check youtub channel kobyoshi02\n",
       "1      1  hey guy check new channel first vid us monkey ...\n",
       "2      1                                 test say murdevcom\n",
       "3      1                     shake sexi ass channel enjoy Ôªø\n",
       "4      1                          watchvvtarggvgtwq check Ôªø\n",
       "5      1  hey check new websit site kid stuff kidsmediau...\n",
       "6      1                                 subscrib channel Ôªø\n",
       "7      0              turn mute soon came want check viewsÔªø\n",
       "8      1                        check channel funni videosÔªø\n",
       "9      1                 u shouldd check channel tell nextÔªø"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[['CLASS', 'text_lower_case']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e983ee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                    text_lower_case\n",
      "1749      1            brazil pleas subscrib channel love allÔªø\n",
      "602       0                           song never get old lt3 Ôªø\n",
      "1329      1  guy check extraordinari websit call zonepacom ...\n",
      "1791      1  hello guysi found way make money onlin get pai...\n",
      "1479      0                                  anybodi els 2015Ôªø\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 stemmed rows.\n",
    "random_sample = combined_df[['CLASS', 'text_lower_case']].sample(n=5)\n",
    "print(random_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203ef2a",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e9173",
   "metadata": {},
   "source": [
    "We can perform lemmatization in two ways. \n",
    "\n",
    "1. Without POS Tagging: It is less accurate as this way, the lemmatizer often assumes that words are nouns which can lead to potential errors.\n",
    "2. With POS Tagging: It is more accurate as it takes the word's role in the sentence into account while performing lemmatizion. Taking POS tags in to account reduces the likelihood of errors. \n",
    "\n",
    "In the following code, we will take up the second process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fb3a3",
   "metadata": {},
   "source": [
    "Note on the approach of coding: POS tags from NLTK‚Äôs are more detailed compared to the broader ones from WordNet. NLTK‚Äôs POS tagger provides the necessary contextual information. Converting NLTK‚Äôs detailed tags to WordNet's simpler POS tags ensures that the lemmatizer has the necessary context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "286b8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Convert NLTK POS tags to WordNet POS tags as explained above.\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Assume nounun if no match is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d0c93e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Note', 'on', 'the', 'approach', 'of', 'coding', ':', 'POS', 'tag', 'from', 'NLTK', '‚Äô', 's', 'be', 'more', 'detailed', 'compare', 'to', 'the', 'broad', 'one', 'from', 'WordNet', '.', 'NLTK', '‚Äô', 's', 'POS', 'tagger', 'provide', 'the', 'necessary', 'contextual', 'information', '.', 'Converting', 'NLTK', '‚Äô', 's', 'detail', 'tag', 'to', 'WordNet', \"'s\", 'simpler', 'POS', 'tag', 'ensure', 'that', 'the', 'lemmatizer', 'have', 'the', 'necessary', 'context', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test on sample text.\n",
    "text = \"\"\"Note on the approach of coding: POS tags from NLTK‚Äôs are more detailed \n",
    "          compared to the broader ones from WordNet. NLTK‚Äôs POS tagger provides the \n",
    "          necessary contextual information. Converting NLTK‚Äôs detailed tags to WordNet's \n",
    "          simpler POS tags ensures that the lemmatizer has the necessary context.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and find the POS tags.\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Initialize the lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize with POS tags\n",
    "lemmatized_words = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2e082c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTENT', 'CLASS', 'text_lower_case', 'frequent_removed',\n",
       "       'rare_removed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b9fe5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will aplly the lemmatize diectly on combined_df['text_lower_case'] as its a necessary step in our case.\n",
    "# First write a function to lemmatize text.\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "49450deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                    text_lower_case\n",
      "0         1         huh anyway check youtub channel kobyoshi02\n",
      "1         1  hey guy check new channel first vid u monkey i...\n",
      "2         1                                 test say murdevcom\n",
      "3         1                      shake sexi as channel enjoy Ôªø\n",
      "4         1                          watchvvtarggvgtwq check Ôªø\n",
      "...     ...                                                ...\n",
      "1951      0                           love song sing camp time\n",
      "1952      0  love song two reason 1it africa 2i born beauti...\n",
      "1953      0                                                wow\n",
      "1954      0                                   shakira u wiredo\n",
      "1955      0                                shakira best dancer\n",
      "\n",
      "[1956 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the lemmatization function directly to combined_df['text_lower_case'].\n",
    "combined_df['text_lower_case'] = combined_df['text_lower_case'].apply(lemmatize_text)\n",
    "\n",
    "# Display the DataFrame with the new lemmatized column\n",
    "print(combined_df[['CLASS','text_lower_case']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836af216",
   "metadata": {},
   "source": [
    "We are half way through yet. We are still left with the following text pre-processing processes that we will directly aply to our main text column, combined_df['text_lower_case']. \n",
    "- Removal of emojis\n",
    "- Removal of emoticons\n",
    "- Conversion of emoticons to words\n",
    "- Conversion of emojis to words\n",
    "- Removal of URLs\n",
    "- Removal of HTML tags\n",
    "- Chat words conversion\n",
    "- Spelling correction\n",
    "\n",
    "Below we will take them up one-by-one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407c0d5",
   "metadata": {},
   "source": [
    "#### Removal of emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f5bb8",
   "metadata": {},
   "source": [
    "In text pre-processing, we need to remove emojis to simplify text data. Removing emojis lets models focus on the most relevant content. Removing emojis helps us to standardize the input data and makes it more uniform and easier to process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44960fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use regular expressions to remove emojis directly from combined_df['text_lower_case'].\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"  # Miscellaneous Symbols\n",
    "        u\"\\U000024C2-\\U0001F251\"  # Enclosed Characters\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d6ddbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                     emojis_removed\n",
      "0         1         huh anyway check youtub channel kobyoshi02\n",
      "1         1  hey guy check new channel first vid u monkey i...\n",
      "2         1                                 test say murdevcom\n",
      "3         1                       shake sexi as channel enjoy \n",
      "4         1                           watchvvtarggvgtwq check \n",
      "...     ...                                                ...\n",
      "1951      0                           love song sing camp time\n",
      "1952      0  love song two reason 1it africa 2i born beauti...\n",
      "1953      0                                                wow\n",
      "1954      0                                   shakira u wiredo\n",
      "1955      0                                shakira best dancer\n",
      "\n",
      "[1956 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function diectly to combined_df['text_lower_case'].\n",
    "combined_df['emojis_removed'] = combined_df['text_lower_case'].apply(remove_emojis)\n",
    "\n",
    "# Pring the output.\n",
    "print(combined_df[['CLASS','emojis_removed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "42f1f1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTENT', 'CLASS', 'text_lower_case', 'frequent_removed',\n",
       "       'rare_removed', 'emojis_removed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d9a20",
   "metadata": {},
   "source": [
    "#### Removal of emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7bab3",
   "metadata": {},
   "source": [
    "You might be wondering the difference between emojis emoticons. Emojis are colourful, digital icons like üòä (smiling face) or üöÄ (rocket) . They stand for objects or emotions. While emoticons, are text-based symbols like :-) (smiley face) or <3 (heart).  Emoticons are created with keyboard characters to express feelings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "715f187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Apply RegEx to remove emoticons. Write a function first.\n",
    "def remove_emoticons(text):\n",
    "    # RegEx pattern to match commonly used emoticons.\n",
    "    emoticon_pattern = re.compile(\n",
    "        r'[:;=8][\\-o\\*]?[)\\]\\(\\[dDpP\\||/\\\\\\^]',\n",
    "        re.UNICODE)\n",
    "    return emoticon_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2e2a96ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hello John! :) How are you? :P I hope you're good today. :D\n",
      "Cleaned Sentence: Hello John!  How are you?  I hope you're good today. \n"
     ]
    }
   ],
   "source": [
    "# Sample sentence with emoticons.\n",
    "sample_sentence = \"Hello John! :) How are you? :P I hope you're good today. :D\"\n",
    "\n",
    "# Remove emoticons from the sample sentence\n",
    "emoticons_removed = remove_emoticons(sample_sentence)\n",
    "\n",
    "# Print the result\n",
    "print(\"Original Sentence:\", sample_sentence)\n",
    "print(\"Cleaned Sentence:\", emoticons_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "be37f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                  emoticons_removed\n",
      "0         1         huh anyway check youtub channel kobyoshi02\n",
      "1         1  hey guy check new channel first vid u monkey i...\n",
      "2         1                                 test say murdevcom\n",
      "3         1                      shake sexi as channel enjoy Ôªø\n",
      "4         1                          watchvvtarggvgtwq check Ôªø\n",
      "...     ...                                                ...\n",
      "1951      0                           love song sing camp time\n",
      "1952      0  love song two reason 1it africa 2i born beauti...\n",
      "1953      0                                                wow\n",
      "1954      0                                   shakira u wiredo\n",
      "1955      0                                shakira best dancer\n",
      "\n",
      "[1956 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the combined_df['text_lower_case']. We will store it in another column for now.\n",
    "combined_df['emoticons_removed'] = combined_df['text_lower_case'].apply(remove_emoticons)\n",
    "\n",
    "# Print the output.\n",
    "print(combined_df[['CLASS','emoticons_removed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "95293677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTENT', 'CLASS', 'text_lower_case', 'frequent_removed',\n",
       "       'rare_removed', 'emojis_removed', 'emoticons_removed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f4959",
   "metadata": {},
   "source": [
    "#### Conversion of emoticons to words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e934c",
   "metadata": {},
   "source": [
    "We can map emoticons to their corresponding descriptions like \"smiley face\" or \"grinning face.\" This conversion helps to preserve the sentiments conveyed by emoticons in the form of simple text words, which are simpler to analyse as compared to plain emoticons. It‚Äôs a sure information gain and it helps to improve the accuracy and effectiveness of the NLP task in hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "54af89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the common emoticons to words.\n",
    "emoticon_to_word = {\n",
    "    ':)': 'smiley face',\n",
    "    ':D': 'grinning face',\n",
    "    ':P': 'playful face',\n",
    "    ':-)': 'smiley face',\n",
    "    ':-D': 'grinning face',\n",
    "    ':-P': 'playful face'\n",
    "}\n",
    "\n",
    "# Function to replace emoticons with words.\n",
    "def emoticons_to_word_converter(text):\n",
    "    for emoticon, word in emoticon_to_word.items():\n",
    "        text = text.replace(emoticon, word)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "010e83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hello there! :) How are you? :P I hope you're doing well. :D\n",
      "Converted Sentence: Hello there! smiley face How are you? playful face I hope you're doing well. grinning face\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence.\n",
    "sample_sentence = \"Hello there! :) How are you? :P I hope you're doing well. :D\"\n",
    "\n",
    "# Convert emoticons to words.\n",
    "converted_sentence = emoticons_to_word_converter(sample_sentence)\n",
    "\n",
    "# Print the output.\n",
    "print(\"Original Sentence:\", sample_sentence)\n",
    "print(\"Converted Sentence:\", converted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "878bb641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                    text_lower_case\n",
      "0         1         huh anyway check youtub channel kobyoshi02\n",
      "1         1  hey guy check new channel first vid u monkey i...\n",
      "2         1                                 test say murdevcom\n",
      "3         1                      shake sexi as channel enjoy Ôªø\n",
      "4         1                          watchvvtarggvgtwq check Ôªø\n",
      "...     ...                                                ...\n",
      "1951      0                           love song sing camp time\n",
      "1952      0  love song two reason 1it africa 2i born beauti...\n",
      "1953      0                                                wow\n",
      "1954      0                                   shakira u wiredo\n",
      "1955      0                                shakira best dancer\n",
      "\n",
      "[1956 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function directly on combined_df['text_lower_case'] \n",
    "# Its likely to be useful in increasing the accuracy of our analysis.\n",
    "\n",
    "# Apply the function to the 'text_lower_case' column\n",
    "combined_df['text_lower_case'] = combined_df['text_lower_case'].apply(convert_emoticons_to_words)\n",
    "\n",
    "# Print the output.\n",
    "print(combined_df[['CLASS', 'text_lower_case']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ccf59",
   "metadata": {},
   "source": [
    "#### Conversion of emojis to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d462ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "                                              0.0/431.4 kB ? eta -:--:--\n",
      "     -------                                 81.9/431.4 kB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------                 256.0/431.4 kB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 431.4/431.4 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\shailendra kadre\\anaconda3\\lib\\site-packages (from emoji) (4.12.2)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.12.1\n"
     ]
    }
   ],
   "source": [
    "# Install emoji if you have not done it earlier. \n",
    "!pip install emoji # it is the necessarylibrary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "28d97103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji \n",
    "\n",
    "# Map of emojis to words.\n",
    "def emojis_to_word_converer(text):\n",
    "    # Convert emojis to their corresponding descriptions.\n",
    "    return emoji.demojize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f0e4071d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello John! üòä How are you? üöÄ I hope you're good today. üéâ\n",
      "Converted Text: Hello John! :smiling_face_with_smiling_eyes: How are you? :rocket: I hope you're good today. :party_popper:\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sample_text = \"Hello John! üòä How are you? üöÄ I hope you're good today. üéâ\"\n",
    "converted_text = emojis_to_word_converer(sample_text)\n",
    "\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Converted Text:\", converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8df5f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                    text_lower_case\n",
      "0         1         huh anyway check youtub channel kobyoshi02\n",
      "1         1  hey guy check new channel first vid u monkey i...\n",
      "2         1                                 test say murdevcom\n",
      "3         1                      shake sexi as channel enjoy Ôªø\n",
      "4         1                          watchvvtarggvgtwq check Ôªø\n",
      "...     ...                                                ...\n",
      "1951      0                           love song sing camp time\n",
      "1952      0  love song two reason 1it africa 2i born beauti...\n",
      "1953      0                                                wow\n",
      "1954      0                                   shakira u wiredo\n",
      "1955      0                                shakira best dancer\n",
      "\n",
      "[1956 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to combined_df['text_lower_case'].\n",
    "combined_df['text_lower_case'] = combined_df['text_lower_case'].apply(emojis_to_word_converer)\n",
    "\n",
    "# Print the output.\n",
    "print(combined_df[['CLASS', 'text_lower_case']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29118978",
   "metadata": {},
   "source": [
    "#### Removal of URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38318105",
   "metadata": {},
   "source": [
    "URLs are noise and irrelevant information for any text analysis. Removing URLs standardizes and cleans the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0a8e855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Let's write a function to remove URLs from the input text.\n",
    "def url_remover(text):\n",
    "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "caffcfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Check out this link: https://www.example.com and also visit http://example.org.\n",
      "Cleaned Text: Check out this link:  and also visit \n"
     ]
    }
   ],
   "source": [
    "# Sample usage below.\n",
    "sample_text = \"Check out this link: https://www.example.com and also visit http://example.org.\"\n",
    "cleaned_text = url_remover(sample_text)\n",
    "\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6350b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                    text_lower_case\n",
      "0         1         huh anyway check youtub channel kobyoshi02\n",
      "1         1  hey guy check new channel first vid u monkey i...\n",
      "2         1                                 test say murdevcom\n",
      "3         1                      shake sexi as channel enjoy Ôªø\n",
      "4         1                          watchvvtarggvgtwq check Ôªø\n",
      "...     ...                                                ...\n",
      "1951      0                           love song sing camp time\n",
      "1952      0  love song two reason 1it africa 2i born beauti...\n",
      "1953      0                                                wow\n",
      "1954      0                                   shakira u wiredo\n",
      "1955      0                                shakira best dancer\n",
      "\n",
      "[1956 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to combined_df['text_lower_case'].\n",
    "combined_df['text_lower_case'] = combined_df['text_lower_case'].apply(url_remover)\n",
    "\n",
    "# Print the output.\n",
    "print(combined_df[['CLASS','text_lower_case']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382a980",
   "metadata": {},
   "source": [
    "#### Removal of HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20c264",
   "metadata": {},
   "source": [
    "HTML tags may pose as noise and irrelavent data in most text analysis. Removing them can improve the accuracy of our text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "25950873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# A simple function to remove HTML tags from the input text.\n",
    "def html_tag_remover(text):\n",
    "    html_tag_pattern = re.compile(r'<[^>]+>')\n",
    "    return html_tag_pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f99cfc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: <p>Hi John! <a href='https://example_url.com'>Click here</a> to visit.</p>\n",
      "Cleaned Text: Hi John! Click here to visit.\n"
     ]
    }
   ],
   "source": [
    "# Sample usage.\n",
    "sample_text = \"<p>Hi John! <a href='https://example_url.com'>Click here</a> to visit.</p>\"\n",
    "cleaned_text = html_tag_remover(sample_text)\n",
    "\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bd004a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                    text_lower_case\n",
      "0         1         huh anyway check youtub channel kobyoshi02\n",
      "1         1  hey guy check new channel first vid u monkey i...\n",
      "2         1                                 test say murdevcom\n",
      "3         1                      shake sexi as channel enjoy Ôªø\n",
      "4         1                          watchvvtarggvgtwq check Ôªø\n",
      "...     ...                                                ...\n",
      "1951      0                           love song sing camp time\n",
      "1952      0  love song two reason 1it africa 2i born beauti...\n",
      "1953      0                                                wow\n",
      "1954      0                                   shakira u wiredo\n",
      "1955      0                                shakira best dancer\n",
      "\n",
      "[1956 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to combined_df['text_lower_case'].\n",
    "combined_df['text_lower_case'] = combined_df['text_lower_case'].apply(html_tag_remover)\n",
    "\n",
    "# Print the output.\n",
    "print(combined_df[['CLASS','text_lower_case']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86576e26",
   "metadata": {},
   "source": [
    "#### Chat words conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5df0c9",
   "metadata": {},
   "source": [
    "Chat words are informal, abbreviated, or slang terms that are popularly used in online messaging, especially by the younger generation. The examples include \"u\" for \"you\" or \"lol\" for \"laughing out loud.\" Converting chat words in to more formal language words can help in accurately analysing them by text processing models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cf80e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map common chat words to more formal words. You can add on to this list.\n",
    "chat_to_formal = {\n",
    "    'u': 'you',\n",
    "    'r': 'are',\n",
    "    'lol': 'laughing out loud',\n",
    "    'brb': 'be right back',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'thx': 'thanks',\n",
    "    'gtg': 'got to go',\n",
    "    'b4': 'before'\n",
    "}\n",
    "\n",
    "# Let's now write a function to replace chat words with formal words.\n",
    "def chat_word_converter(text):\n",
    "    for chat_word, formal_word in chat_to_formal.items():\n",
    "        text = text.replace(chat_word, formal_word)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "daa8b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hey John! r u coming to the function this evening? lol, thx for your invite!\n",
      "Converted Text: Hey John! are you coming to the fyounction this evening? laughing out loud, thanks foare yoyouare invite!\n"
     ]
    }
   ],
   "source": [
    "# Sample usage.\n",
    "sample_text = \"Hey John! r u coming to the function this evening? lol, thx for your invite!\"\n",
    "converted_text = chat_word_converter(sample_text)\n",
    "\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Converted Text:\", converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "46b22097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CLASS                                    text_lower_case\n",
      "0         1   hyouh anyway check yoyoutyoub channel kobyoshi02\n",
      "1         1  hey gyouy check new channel fiarest vid you mo...\n",
      "2         1                             test say myouaredevcom\n",
      "3         1                      shake sexi as channel enjoy Ôªø\n",
      "4         1                        watchvvtaareggvgtwq check Ôªø\n",
      "...     ...                                                ...\n",
      "1951      0                           love song sing camp time\n",
      "1952      0  love song two areeason 1it afareica 2i boaren ...\n",
      "1953      0                                                wow\n",
      "1954      0                             shakiarea you wiareedo\n",
      "1955      0                            shakiarea best danceare\n",
      "\n",
      "[1956 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to combined_df['text_lower_case'].\n",
    "combined_df['text_lower_case'] = combined_df['text_lower_case'].apply(chat_word_converter)\n",
    "\n",
    "# Print the output.\n",
    "print(combined_df[['CLASS','text_lower_case']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fa7f6",
   "metadata": {},
   "source": [
    "#### Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24406e1",
   "metadata": {},
   "source": [
    "Spelling correction will increase the information gain and help in accurate analysis of the input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d05eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SpellChecker # Necesary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9095a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install indexer # Another necesary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4032e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspellchecker # Another necesary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "93d4ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function to correct spelling in text\n",
    "def spelling_corrector(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    corrected_words = [spell.candidates(word).pop() if spell.candidates(word) else word for word in words]\n",
    "    return ' '.join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "efa1c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I havv a speling mistke in this sentnce.', \n",
      "                            'Anothr exampl with spellig erors.', \n",
      "                            'No spellig errors here!\n",
      "               \n",
      "Converted Text: I have a spieling mistake in this sentnce.', another example with spelling erors.', no spelling errors heres\n"
     ]
    }
   ],
   "source": [
    "# Sample usage.\n",
    "sample_text = \"\"\"We havv few speling mistkes in this short paragraph.', \n",
    "                            'Anothr exampl with spellig erors.', \n",
    "                            'No spellig errors here!\n",
    "               \"\"\"\n",
    "converted_text = spelling_corrector(sample_text)\n",
    "\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Converted Text:\", converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to combined_df['text_lower_case'].\n",
    "combined_df['text_lower_case'] = combined_df['text_lower_case'].apply(spelling_corrector)\n",
    "\n",
    "# Print the output. \n",
    "print(combined_df[['CLASS','text_lower_case']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff9ea8",
   "metadata": {},
   "source": [
    "Note: Applying the spelling_corrector function to the combined_df['text_lower_case'] was taking a long time. So, I aborted the kernel. You can try it on a better machine or cloud. I tried TextBlob as well, but it was also taking a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1189e965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTENT', 'CLASS', 'text_lower_case', 'frequent_removed',\n",
       "       'rare_removed', 'emojis_removed', 'emoticons_removed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead3e05",
   "metadata": {},
   "source": [
    "- The useful columns for further processing are 'CLASS' and 'text_lower_case'. All others are for demo. Use can use their code in your projects dpending upon what type of text analysis you are trying to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05efe7",
   "metadata": {},
   "source": [
    "#### Apply Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7358d3",
   "metadata": {},
   "source": [
    "- Now all the below steps are similar to what we have seen in Chapter 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "23267d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate features and the target.\n",
    "X = np.array(combined_df['text_lower_case'])\n",
    "y = np.array(combined_df['CLASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "17e7856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer() # Instrantiate CountVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0d8f65ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1956,)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b2108d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in to train and test datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f2efbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use count_vectorizer.fit_transform() for X_train and X_test.\n",
    "X_train = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cb05f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vectorizer.transform(X_test) # We will do onlt transform() with X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fcef0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6e218b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87470449, 0.9044289 , 0.90487239])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, X_train, y_train, cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec5a25",
   "metadata": {},
   "source": [
    "Note: Surprisingly we got considerably better accuracies in Chapter 1, in which mini,al pre-processing was done. Looks like there is a considerable loss of information in these pre-processing steps. It's an interesting lesson for all of us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "15c33b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train) # Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "66adf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the test data. \n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b9d46818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataframe with columns as y_test and y_pred. \n",
    "test_df = pd.DataFrame()\n",
    "test_df[\"y\"] = y_test\n",
    "test_df[\"y_predict\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "682a0f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y  y_predict\n",
      "9    1          1\n",
      "218  1          1\n",
      "3    1          1\n",
      "362  1          1\n",
      "291  1          1\n",
      "174  0          0\n",
      "99   1          1\n",
      "357  0          0\n",
      "531  0          0\n",
      "294  1          1\n"
     ]
    }
   ],
   "source": [
    "# Display 10 random rows from test_df\n",
    "random_sample = test_df.sample(n=10)\n",
    "print(random_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677f773",
   "metadata": {},
   "source": [
    "- The ground truth, y_test, till comares good with the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214c8e4",
   "metadata": {},
   "source": [
    "#### References (the dataset source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6489dc1",
   "metadata": {},
   "source": [
    "[1] Alberto,T.C. and Lochter,J.V.. (2017). YouTube Spam Collection. UCI Machine Learning Repository. https://doi.org/10.24432/C58885."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6c29d",
   "metadata": {},
   "source": [
    "Code Snippet 5.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
