{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b487067",
   "metadata": {},
   "source": [
    "### Tokenisation Revisited (concepts and code demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748418d",
   "metadata": {},
   "source": [
    "#### 1. Whitespace Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33943c0",
   "metadata": {},
   "source": [
    "Whitespace tokenization breaks a text document based on spaces and line breaks. It considers every section of input text divided by whitespace as a distinct token. Whitespace tokenization serves as a quick way to break down text into words or phrases. This method is especially used for processing structured data including logs files or code, in which whitespaces have specific meaning.\n",
    "\n",
    "Whitespace tokenization is less worried about the grammatical structure of input texts. Its main focus is on the visual separation of text. Whitespace tokenization is used in the NLP tasks where sentence boundaries are not as important. Examples include keyword extraction or simple text pre-processing, where the focus is on quickly splitting the input text into smaller and more manageable tasks. Below is the code demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636cfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd7f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from typing import List\n",
    "\n",
    "def whitespace_tokenizer(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text based on whitespace characters (spaces, tabs, newlines).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The input text to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        A list of tokens.\n",
    "    \"\"\"\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b13aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Whitespace', 'tokenization', 'breaks', 'a', 'text', 'document', 'based', 'on', 'spaces', 'and', 'line', 'breaks.', 'It', 'considers', 'every', 'section', 'of', 'input', 'text', 'divided', 'by', 'whitespace', 'as', 'a', 'distinct', 'token.', 'Whitespace', 'tokenization', 'serves', 'as', 'a', 'quick', 'way', 'to', 'break', 'down', 'text', 'into', 'words', 'or', 'phrases.', 'This', 'method', 'is', 'especially', 'used', 'for', 'processing', 'structured', 'data', 'including', 'logs', 'files', 'or', 'code,', 'in', 'which', 'whitespaces', 'have', 'specific', 'meaning.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"Whitespace tokenization breaks a text document based on spaces and line breaks. \n",
    "    It considers every section of input text divided by whitespace as a distinct token. \n",
    "    Whitespace tokenization serves as a quick way to break down text into words or phrases. \n",
    "    This method is especially used for processing structured data including logs files or code, \n",
    "    in which whitespaces have specific meaning.\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = whitespace_tokenize(sample_text)\n",
    "    print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc81341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50324f4e",
   "metadata": {},
   "source": [
    "#### 2. Dictionary Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41340a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97eb645c",
   "metadata": {},
   "source": [
    "In Dictionary-based tokenization, the tokens are based on the words that are already available in the dictionary. In case the token is not found in dictionary, special rules are applied to tokenize it. \n",
    "\n",
    "Dictionary-based tokenization is used where identifying extract predefined phrases or terms is more important. It ensures used in domain-specific texts, where consistency in recognizing specific tokens is more important than simple tokens. This method is particularly useful in NLP tasks like medical text processing and parsing of legal documents, where precise recognition term is critical. Belos is the code demo of dictionary-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2915d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from typing import List\n",
    "\n",
    "# NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "def dictionary_based_tokenizer(text: str, dictionary: List[List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text based on a predefined dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The input text string.\n",
    "    dictionary : List[List[str]]\n",
    "        A list of expressions, recognized as single tokens.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        A list of tokens.\n",
    "    \"\"\"\n",
    "    # The '_' argument joinS the words with an underscore (_).\n",
    "    tokenizer = MWETokenizer(dictionary, separator='_')\n",
    "    tokens = tokenizer.tokenize(nltk.word_tokenize(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c594b2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['John', 'suffered', 'a', 'messive', 'heart_attack', '.', 'He', 'was', 'enroled', 'in', 'machine_learning', 'in', 'Washington', 'DC', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a dictionary of multi-word expressions\n",
    "    dictionary = [\n",
    "        ['heart', 'attack'],\n",
    "        ['machine', 'learning'],\n",
    "        ['New', 'York'],\n",
    "    ]\n",
    "    \n",
    "    sample_text = \"John suffered a messive heart attack. He was enroled in machine learning in Washington DC.\"\n",
    "    tokens = dictionary_based_tokenizer(sample_text, dictionary)\n",
    "    print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bc9e4",
   "metadata": {},
   "source": [
    "Note: You may notice in the output that 'heart attack' is appearing as 'heart_attack' and 'machine learning' as 'machine_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3671983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af213864",
   "metadata": {},
   "source": [
    "#### 3. Rule Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8211e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eed8a72c",
   "metadata": {},
   "source": [
    "Rule-based tokenization is particularly useful in the text pre-processing tasks where precise structures or formats must be preserved. Regular expressions can be used to fine-tune to handle various linguistic nuances or specific text formats. Below we have used RegexpTokenizer from the nltk along with required regular expression patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60344fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from typing import List\n",
    "\n",
    "def rule_based_tokenizer(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text based on regular expressions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The input string.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        A list of tokens.\n",
    "    \"\"\"\n",
    "    # Define a regular expressions for specific tasks.\n",
    "    # Rule 1: Match abbreviations like \"U.K.\", \"Dr.\"\n",
    "    # Rule 2: Match words including those with apostrophes like \"she's\", \"haven't\")\n",
    "    # Rule 3: Separately match punctuations. \n",
    "    pattern = r'''(?x)               # Set flag to allow verbose regex\n",
    "                  (?:[A-Za-z]\\.)+    # Abbreviations like U.K.\n",
    "                | \\w+(?:'\\w+)?       # Words with apostrophes\n",
    "                | [^\\w\\s]            # Separate punctuations\n",
    "                '''\n",
    "    \n",
    "    # Write a RegexpTokenizer.\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    \n",
    "    # Tokenize.\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1908fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Dr', '.', \"John's\", 'a', 'rocket', 'engineer', 'in', 'the', 'U.K.', 'U.A.E.', 'govt', '.', 'has', 'awarded', 'him', 'many', 'awards', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"Dr. John's a rocket engineer in the U.K. \n",
    "                   U.A.E. govt. has awarded him many awards.\"\"\"\n",
    "    tokens = rule_based_tokenizer(sample_text)\n",
    "    print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0a77a",
   "metadata": {},
   "source": [
    "- Please note, in 'Dr', 'U.K.', and 'U.A.E.', the tokenizer has preserved common abbreviations and acronyms and  treated them as single tokens. \n",
    "- 'govt': This is treated as a single token, preserving abbreviations that are commonly used in written English.\n",
    "- For \"John's\", the tokenizer preserves the apostrophe and the following 's' as part of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88799c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11d01a41",
   "metadata": {},
   "source": [
    "#### 4. Punctuation-Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561690d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76220c28",
   "metadata": {},
   "source": [
    "This tokenizer splits the input text on whitespace and punctuations while retaining the punctuations.Punctuation-based tokenization overcomes the issue above and provides a meaningful token.Punctuation-based tokenization solves the problem of simply splitting the input text on whitespace. Below is a simple code demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef809154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e69f27b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr',\n",
       " '.',\n",
       " 'Johnson',\n",
       " 'owns',\n",
       " 'three',\n",
       " 'houses',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'on',\n",
       " ':',\n",
       " '22nd',\n",
       " 'street',\n",
       " ',',\n",
       " 'downton',\n",
       " ',',\n",
       " 'and',\n",
       " '42nd',\n",
       " 'street',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import wordpunct_tokenize from nltk.\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"Mr.Johnson owns three houses in New York on: 22nd street, downton,and 42nd street.\"\n",
    "\n",
    "tokens = wordpunct_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500813f5",
   "metadata": {},
   "source": [
    "- Please note: The above output retains punctuation marks as separate tokens. It helps in understanding the sentence structure. The presence of punctuation tokens help to preserve the meaning and context of the original sentence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af266d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b297c26e",
   "metadata": {},
   "source": [
    "#### 5. Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab6ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c124d952",
   "metadata": {},
   "source": [
    "Tweets can be treated as special texts like several others because they have a typical structure. The generic tokenizers may fail to produce feasible tokens when applied to the datasets containing tweets. nltk has a special tokenizer for such cases. The Tweet tokenizer is a rule-based tokenizer that can be used to remove problematic characters, HTML code, and Twitter handles. This tokenizer can normalize text length of input text by reducing the occurrence of repeated letters. Additionally, tweet tokenizer is equipped to effectively deal with hashtags and mentions while preserving their significance. The tokenizer can also effectively manage emoticons and special symbols. Note that hash tags and emoticons are common in tweets. Below is a small but reusable code demo with this tokenizer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03f24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d0f297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def punctuation_based_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using NLTK's TweetTokenizer, which handles punctuation and special characters\n",
    "    commonly found in tweets.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens, including words and punctuation marks.\n",
    "    \"\"\"\n",
    "    # Initialize the tokenizer.\n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    # Tokenize. \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3eb0b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.Johnson', 'owns', 'three', 'houses', 'in', 'New', 'York', '!', '22nd', 'street', ',', 'downton', ',', 'and', '42nd', 'street', '.', '\"', '#marvellous', '@Rita', '\"']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    Sample_text = \"\"\"Mr.Johnson owns three houses in New York! \n",
    "    22nd street, downton,and 42nd street.\" #marvellous @Rita\"\n",
    "    \"\"\"\n",
    "    tokens = punctuation_based_tokenizer(Sample_text)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46409ff",
   "metadata": {},
   "source": [
    "- Please note: Punctuation marks like !, ,, and . are preserved as separate tokens. \n",
    "- Hashtags (#marvellous) and mentions (@Rita), are preserved as distinct tokens.\n",
    "- Mr.Johnson is kept intact.\n",
    "- These are the examples to put an emphasis on the fact that TweetTokenizer is specifically tailored for the nuances of social media text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee11d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97fbd492",
   "metadata": {},
   "source": [
    "#### 6. Multi-Word Expression (MWE) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb6b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac71d4b8",
   "metadata": {},
   "source": [
    "MWE (Multi-Word Expression) Tokenizer handles sequences of words that act as a single unit in the input text. Examples include idiomatic phrases, compound nouns, and fixed expressions that can be misunderstood if tokenized into separate words. Treating these expressions single tokens, preserves their intended meaning and helps in accurate text analysis. Below is the text demo of this tokenizer with a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e60e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "871d7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "def mwe_tokenizer(text, mwe_list):\n",
    "    \"\"\"\n",
    "    Tokenizes while preserving indicated multi-word expressions (MWEs).\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text.\n",
    "    mwe_list (list of tuples): The list of MWEs to be treated as single tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens with MWEs preserved as single tokens.\n",
    "    \"\"\"\n",
    "    # Initialize the MWETokenizer with the provided MWEs\n",
    "    tokenizer = MWETokenizer(mwe_list)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.tokenize(text.split())\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b221c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define multi-word expressions.\n",
    "    mwe_list = [\n",
    "        ('M', 'W', 'E'),\n",
    "        ('Multi', 'Word', 'Tokenizer'),\n",
    "        ('Natural', 'Language', 'Processing'),\n",
    "        ('pre', 'processing')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1705bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M_W_E', 'Tokenizer', 'is', 'an', 'advanced', 'tokenizer', 'in', 'Natural_Language_Processing', 'pre_processing', 'tasks']\n"
     ]
    }
   ],
   "source": [
    " # Input text\n",
    "text = \"M W E Tokenizer is an advanced tokenizer in Natural Language Processing pre processing tasks\"\n",
    "    \n",
    "# Perform tokenization with MWEs\n",
    "tokenized = mwe_tokenizer(text, mwe_list)\n",
    "    \n",
    "# Output the tokenized result\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcbfef",
   "metadata": {},
   "source": [
    "- Note that M_W_E, Natural Language Processing, and pre processing are combined as single units as per the given mwe_list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe2f20",
   "metadata": {},
   "source": [
    "- More task specific tokenizers are available like Penn TreeBank/Default Tokenization, Spacy Tokenizer, Moses Tokenizer, Subword Tokenization. The readers are expected to explore them using other books and web resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e4cf3",
   "metadata": {},
   "source": [
    "Code Snippet 5.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
